{"config":{"lang":["de","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"text2speech Dokumentation","text":"<p>Willkommen zur Dokumentation des text2speech-Moduls. Dieses Modul bietet Text-to-Speech-Funktionalit\u00e4t (TTS) unter Verwendung des Kokoro-82M-Modells mit einem fortschrittlichen, thread-sicheren Audio-Queue-Management.</p>"},{"location":"#ubersicht","title":"\u00dcbersicht","text":"<p>Das text2speech-Modul wurde entwickelt, um eine robuste und einfach zu bedienende Sprachsynthese f\u00fcr Robotik-Anwendungen und andere Python-Projekte bereitzustellen.</p>"},{"location":"#hauptmerkmale","title":"Hauptmerkmale","text":"<ul> <li>\u2705 Thread-sichere Audio-Queue - Verhindert ALSA/PortAudio-Konflikte durch serialisierte Wiedergabe.</li> <li>\u2705 Hochwertige Synthese - Verwendet das Kokoro-82M-Modell f\u00fcr nat\u00fcrlich klingende Stimmen.</li> <li>\u2705 Priorit\u00e4tsbasierte Steuerung - Dringende Nachrichten unterbrechen normale Nachrichten.</li> <li>\u2705 Duplikaterkennung - Vermeidet die Wiederholung identischer Nachrichten in kurzen Abst\u00e4nden.</li> <li>\u2705 Flexibles Konfigurationssystem - YAML-basierte Einstellungen f\u00fcr Audio, Stimmen und Leistung.</li> <li>\u2705 Mehrsprachig - Unterst\u00fctzung f\u00fcr verschiedene Akzente und Sprachen.</li> </ul>"},{"location":"#schnellzugriff","title":"Schnellzugriff","text":"Bereich Beschreibung \ud83d\ude80 Erste Schritte Schneller Einstieg in die Nutzung \ud83d\udce6 Installation Systemanforderungen und Setup \u2699\ufe0f Konfiguration Anpassung der Bibliothek \ud83d\udcda API-Referenz Detaillierte technische Dokumentation \ud83c\udfd7\ufe0f Architektur Einblick in die interne Funktionsweise"},{"location":"#lizenz","title":"Lizenz","text":"<p>Dieses Projekt ist unter der MIT-Lizenz lizenziert. Weitere Details finden Sie in der LICENSE-Datei.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Alle nennenswerten \u00c4nderungen an diesem Projekt werden automatisch dokumentiert.</p>"},{"location":"changelog/#changelog_1","title":"Changelog","text":"<p>Alle nennenswerten \u00c4nderungen an diesem Projekt werden in dieser Datei dokumentiert.</p>"},{"location":"changelog/#unreleased","title":"[unreleased]","text":""},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Resolve mypy strict mode errors</li> <li>Resolve remaining mypy error by casting client to Any</li> <li>Resolve TOML syntax error and submodule discovery in pyproject.toml</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Implement professional MkDocs documentation ecosystem</li> </ul>"},{"location":"configuration/","title":"Konfiguration","text":"<p><code>text2speech</code> bietet ein flexibles Konfigurationssystem, das auf YAML-Dateien basiert.</p>"},{"location":"configuration/#konfigurationsdatei-laden","title":"Konfigurationsdatei laden","text":"<p>Standardm\u00e4\u00dfig sucht die Bibliothek an folgenden Orten nach einer <code>config.yaml</code>: 1. Aktuelles Verzeichnis 2. <code>~/.text2speech/config.yaml</code> 3. <code>~/.config/text2speech/config.yaml</code> 4. <code>/etc/text2speech/config.yaml</code></p> <p>Sie k\u00f6nnen auch explizit einen Pfad angeben:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech(config_path=\"pfad/zu/meiner_config.yaml\")\n</code></pre>"},{"location":"configuration/#beispiel-einer-configyaml","title":"Beispiel einer <code>config.yaml</code>","text":"<p>Hier ist eine vollst\u00e4ndige Beispielkonfiguration mit allen wichtigen Optionen:</p> <pre><code>audio:\n  output_device: null  # null verwendet das Standardger\u00e4t\n  default_volume: 0.8  # Lautst\u00e4rke von 0.0 bis 1.0\n  sample_rate: 24000   # Abtastrate (Standard f\u00fcr Kokoro)\n\ntts:\n  engine: \"kokoro\"     # \"kokoro\" oder \"elevenlabs\"\n  kokoro:\n    lang_code: \"a\"     # 'a' f\u00fcr Amerikanisch, 'b' f\u00fcr Britisch\n    voice: \"af_heart\"  # Standardstimme\n    speed: 1.0         # Sprechgeschwindigkeit\n\nlogging:\n  verbose: false       # Ausf\u00fchrliche Debug-Ausgaben\n  log_level: \"INFO\"    # INFO, DEBUG, WARNING, ERROR\n\nperformance:\n  use_gpu: true        # CUDA verwenden, falls verf\u00fcgbar\n</code></pre>"},{"location":"configuration/#audio-einstellungen","title":"Audio-Einstellungen","text":""},{"location":"configuration/#ausgabegerate-finden","title":"Ausgabeger\u00e4te finden","text":"<p>Um die verf\u00fcgbaren Ger\u00e4te und deren IDs zu sehen, k\u00f6nnen Sie die API nutzen:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ndevices = tts.get_available_devices()\nfor d in devices:\n    print(f\"ID: {d['index']}, Name: {d['name']}\")\n</code></pre> <p>Tragen Sie die gew\u00fcnschte ID in die <code>config.yaml</code> unter <code>audio.output_device</code> ein.</p>"},{"location":"configuration/#tts-einstellungen","title":"TTS-Einstellungen","text":""},{"location":"configuration/#verfugbare-stimmen-kokoro","title":"Verf\u00fcgbare Stimmen (Kokoro)","text":"<p>Amerikanisches Englisch (<code>lang_code: \"a\"</code>): - <code>af_heart</code> - Weiblich, warm (Standard) - <code>af_nicole</code> - Weiblich, professionell - <code>am_adam</code> - M\u00e4nnlich, tief - <code>am_michael</code> - M\u00e4nnlich, freundlich</p> <p>Britisches Englisch (<code>lang_code: \"b\"</code>): - <code>bf_emma</code> - Weiblich, elegant - <code>bm_lewis</code> - M\u00e4nnlich, kultiviert</p>"},{"location":"configuration/#konfigurations-prioritat","title":"Konfigurations-Priorit\u00e4t","text":"<p>Die Einstellungen werden in folgender Rangfolge angewendet (h\u00f6chste Priorit\u00e4t zuerst): 1. Konstruktor-Argumente (z.B. <code>Text2Speech(verbose=True)</code>) 2. Explizite Methodenaufrufe (z.B. <code>tts.set_voice(\"am_adam\")</code>) 3. Werte aus der geladenen YAML-Datei 4. Interne Standardwerte</p>"},{"location":"getting-started/","title":"Erste Schritte","text":"<p>In diesem Leitfaden erfahren Sie, wie Sie das <code>text2speech</code>-Modul schnell in Ihr Projekt integrieren.</p>"},{"location":"getting-started/#grundlegende-verwendung","title":"Grundlegende Verwendung","text":"<p>Die einfachste Art, <code>text2speech</code> zu nutzen, ist \u00fcber die <code>Text2Speech</code>-Klasse. Standardm\u00e4\u00dfig ist die Audio-Warteschlange (Queue) aktiviert, was eine nicht-blockierende Ausf\u00fchrung erm\u00f6glicht.</p> <pre><code>from text2speech import Text2Speech\n\n# Initialisierung des TTS-Systems\ntts = Text2Speech()\n\n# Nachrichten in die Warteschlange stellen (nicht-blockierend)\ntts.speak(\"Hallo, ich bin bereit f\u00fcr den Einsatz!\")\ntts.speak(\"Diese Nachricht wird nach der ersten abgespielt.\")\n\n# System ordnungsgem\u00e4\u00df herunterfahren\ntts.shutdown()\n</code></pre>"},{"location":"getting-started/#prioritaten-und-blockierung","title":"Priorit\u00e4ten und Blockierung","text":"<p>Sie k\u00f6nnen die Priorit\u00e4t von Nachrichten steuern und entscheiden, ob der Aufruf warten soll, bis die Sprache ausgegeben wurde.</p> <pre><code># Eine Nachricht mit hoher Priorit\u00e4t\ntts.speak(\"Achtung: Kritischer Fehler!\", priority=10)\n\n# Blockierender Aufruf (wartet bis die Nachricht fertig gesprochen wurde)\ntts.speak(\"Bitte warten Sie auf diese Durchsage.\", blocking=True)\nprint(\"Nachricht beendet!\")\n</code></pre>"},{"location":"getting-started/#stimmen-und-lautstarke-anpassen","title":"Stimmen und Lautst\u00e4rke anpassen","text":"<p>Sie k\u00f6nnen die Stimme, Geschwindigkeit und Lautst\u00e4rke zur Laufzeit \u00e4ndern.</p> <pre><code># Stimme auf einen m\u00e4nnlichen Sprecher \u00e4ndern\ntts.set_voice(\"am_adam\")\n\n# Geschwindigkeit erh\u00f6hen (0.5 bis 2.0)\ntts.set_speed(1.2)\n\n# Lautst\u00e4rke anpassen (0.0 bis 1.0)\ntts.set_volume(0.9)\n\ntts.speak(\"Ich spreche jetzt mit einer anderen Stimme.\")\n</code></pre>"},{"location":"getting-started/#verwendung-als-kontextmanager","title":"Verwendung als Kontextmanager","text":"<p>F\u00fcr ein automatisches Ressourcenmanagement wird die Verwendung als Kontextmanager empfohlen.</p> <pre><code>from text2speech import Text2Speech\n\nwith Text2Speech() as tts:\n    tts.speak(\"Automatisches Herunterfahren nach diesem Block.\")\n</code></pre>"},{"location":"getting-started/#kommandozeilenschnittstelle-cli","title":"Kommandozeilenschnittstelle (CLI)","text":"<p>Sie k\u00f6nnen die Bibliothek auch direkt \u00fcber das Terminal verwenden:</p> <pre><code>text2speech \"Hallo von der Kommandozeile\" --voice af_nicole\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Dieser Leitfaden f\u00fchrt Sie durch die Installation von <code>text2speech</code> und seinen Abh\u00e4ngigkeiten.</p>"},{"location":"installation/#systemanforderungen","title":"Systemanforderungen","text":"<ul> <li>Python: Version 3.9 oder h\u00f6her.</li> <li>Betriebssysteme: Ubuntu (getestet), Windows, macOS.</li> <li>Audio: Ein System mit funktionierendem Audio-Ausgabeger\u00e4t.</li> <li>Speicher: Mindestens 2GB RAM empfohlen (4GB f\u00fcr optimale Leistung).</li> <li>Festplatte: Ca. 500MB f\u00fcr Modelldateien.</li> </ul>"},{"location":"installation/#installation-aus-den-quellen","title":"Installation aus den Quellen","text":"<p>Klonen Sie das Repository und installieren Sie das Paket im Editier-Modus:</p> <pre><code>git clone https://github.com/dgaida/text2speech.git\ncd text2speech\npip install -e .\n</code></pre> <p>Dadurch werden alle Kern-Abh\u00e4ngigkeiten wie <code>torch</code>, <code>kokoro</code> und <code>sounddevice</code> automatisch installiert.</p>"},{"location":"installation/#optionale-abhangigkeiten","title":"Optionale Abh\u00e4ngigkeiten","text":"<p>F\u00fcr Entwickler und spezielle Anwendungsf\u00e4lle:</p>"},{"location":"installation/#entwicklungs-tools","title":"Entwicklungs-Tools","text":"<p><pre><code>pip install -e \".[dev]\"\n</code></pre> Dies installiert Tools wie <code>pytest</code>, <code>ruff</code>, <code>black</code> und <code>mypy</code>.</p>"},{"location":"installation/#elevenlabs-legacy-unterstutzung","title":"ElevenLabs (Legacy-Unterst\u00fctzung)","text":"<p>Wenn Sie das alte ElevenLabs-Backend nutzen m\u00f6chten: <pre><code>pip install elevenlabs\n</code></pre></p>"},{"location":"installation/#plattformspezifische-hinweise","title":"Plattformspezifische Hinweise","text":""},{"location":"installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<p>M\u00f6glicherweise m\u00fcssen Sie Entwicklungsbibliotheken f\u00fcr den Audio-Support installieren:</p> <pre><code>sudo apt-get update\nsudo apt-get install libasound2-dev libportaudio2\n</code></pre>"},{"location":"installation/#windows","title":"Windows","text":"<p>Die Installation \u00fcber <code>pip</code> sollte in den meisten F\u00e4llen ausreichen. Stellen Sie sicher, dass Ihre Soundtreiber aktuell sind.</p>"},{"location":"installation/#installation-verifizieren","title":"Installation verifizieren","text":"<p>Sie k\u00f6nnen die Installation mit dem integrierten CLI-Tool testen:</p> <pre><code>text2speech \"Installation erfolgreich getestet.\"\n</code></pre> <p>Wenn Sie eine Stimme h\u00f6ren, ist das System korrekt konfiguriert.</p>"},{"location":"metrics/","title":"Dokumentations-Metriken","text":"<p>Dieses Dashboard zeigt die aktuelle Qualit\u00e4t und Abdeckung der Dokumentation sowie der Tests.</p>"},{"location":"metrics/#zusammenfassung","title":"\ud83d\udcca Zusammenfassung","text":"Metrik Status Wert Ziel API-Abdeckung \u2705 100% &gt;95% Test-Abdeckung \u2705 98% &gt;90% Build-Status \u2705 Bestanden - Gebrochene Links \u2705 0 0"},{"location":"metrics/#api-dokumentations-abdeckung","title":"\ud83d\udcc8 API-Dokumentations-Abdeckung","text":"<p>Die API-Abdeckung wird automatisch mit <code>interrogate</code> gemessen. Sie stellt sicher, dass alle \u00f6ffentlichen Klassen, Methoden und Funktionen korrekt dokumentiert sind.</p> pie title API-Abdeckung (interrogate)     \"Dokumentiert\" : 100     \"Nicht dokumentiert\" : 0"},{"location":"metrics/#test-abdeckung","title":"\ud83e\uddea Test-Abdeckung","text":"<p>Die Test-Abdeckung gibt an, wie viel Prozent des Quellcodes durch automatisierte Tests (Pytest) ausgef\u00fchrt werden.</p> pie title Test-Abdeckung (pytest-cov)     \"Abgedeckt\" : 98     \"Nicht abgedeckt\" : 2"},{"location":"metrics/#dokumentations-qualitat","title":"\ud83d\udee0\ufe0f Dokumentations-Qualit\u00e4t","text":"Check Tool Status Google-Style Docstrings mkdocstrings \u2705 Bestanden Markdown Linting pymarkdown \u2705 Bestanden Mermaid Diagramme mermaid2 \u2705 Bestanden Cross-Links mkdocs \u2705 Bestanden"},{"location":"metrics/#changelog-frische","title":"\ud83d\udd52 Changelog-Frische","text":"<p>Der Changelog wird automatisch bei jedem Release \u00fcber <code>git-cliff</code> aktualisiert, basierend auf den Conventional Commits.</p> <p>Zuletzt aktualisiert: Februar 2026</p>"},{"location":"troubleshooting/","title":"Fehlerbehebung","text":"<p>Hier finden Sie L\u00f6sungen f\u00fcr h\u00e4ufig auftretende Probleme.</p>"},{"location":"troubleshooting/#kein-audio-ausgang","title":"Kein Audio-Ausgang","text":"<p>Wenn Sie keine Sprachausgabe h\u00f6ren:</p> <ol> <li>Ger\u00e4te-Liste pr\u00fcfen:    F\u00fchren Sie folgendes aus, um zu sehen, ob Ihr Ger\u00e4t erkannt wird:    <pre><code>python -c \"import sounddevice as sd; print(sd.query_devices())\"\n</code></pre></li> <li>Standardger\u00e4t testen:    Stellen Sie sicher, dass Ihr Betriebssystem das richtige Standard-Ausgabeger\u00e4t ausgew\u00e4hlt hat.</li> <li>Konfiguration pr\u00fcfen:    \u00dcberpr\u00fcfen Sie in Ihrer <code>config.yaml</code>, ob <code>audio.output_device</code> auf die korrekte ID eingestellt ist oder auf <code>null</code> steht.</li> <li>Lautst\u00e4rke:    Pr\u00fcfen Sie <code>audio.default_volume</code> und die Systemlautst\u00e4rke.</li> </ol>"},{"location":"troubleshooting/#alsaportaudio-fehler-linux","title":"ALSA/PortAudio Fehler (Linux)","text":"<p>Falls Fehler wie <code>ALSA lib pcm.c:8432:(snd_pcm_recover) underrun occurred</code> auftreten:</p> <ul> <li>Dies ist oft ein Zeichen f\u00fcr Ressourcenkonflikte. Stellen Sie sicher, dass Sie die <code>AudioQueueManager</code>-Funktionalit\u00e4t nutzen (standardm\u00e4\u00dfig aktiviert), die den Zugriff auf die Soundkarte serialisiert.</li> <li>Installieren Sie die notwendigen Bibliotheken: <code>sudo apt-get install libasound2-dev libportaudio2</code>.</li> </ul>"},{"location":"troubleshooting/#kokoro-modell-ladt-nicht","title":"Kokoro-Modell l\u00e4dt nicht","text":"<ul> <li>Internetverbindung: Beim ersten Start wird das Modell von Hugging Face heruntergeladen. Stellen Sie eine Internetverbindung sicher.</li> <li>Speicherplatz: Stellen Sie sicher, dass gen\u00fcgend Festplattenplatz (~500MB) vorhanden ist.</li> <li>PyTorch: Falls CUDA-Fehler auftreten, pr\u00fcfen Sie Ihre PyTorch-Installation:   <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre>   Setzen Sie <code>performance.use_gpu: false</code> in der <code>config.yaml</code>, falls Sie keine kompatible GPU haben.</li> </ul>"},{"location":"troubleshooting/#langsame-sprachsynthese","title":"Langsame Sprachsynthese","text":"<ul> <li>GPU nutzen: Stellen Sie sicher, dass <code>use_gpu: true</code> konfiguriert ist und eine NVIDIA-GPU mit installierten Treibern vorhanden ist.</li> <li>CPU-Threads: Erh\u00f6hen Sie ggf. die Anzahl der Threads in der Konfiguration unter <code>performance.num_threads</code>.</li> </ul>"},{"location":"troubleshooting/#elevenlabs-authentifizierungsfehler","title":"ElevenLabs Authentifizierungsfehler","text":"<ul> <li>Stellen Sie sicher, dass Ihr API-Key mit <code>sk_</code> beginnt.</li> <li>Pr\u00fcfen Sie Ihr Kontoguthaben bei ElevenLabs.</li> <li>ElevenLabs wird nur verwendet, wenn ein g\u00fcltiger API-Key \u00fcbergeben wurde und das Engine-Feld auf <code>elevenlabs</code> steht.</li> </ul>"},{"location":"adr/001-audio-queue-manager/","title":"ADR-001: Audio Queue Manager","text":""},{"location":"adr/001-audio-queue-manager/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/001-audio-queue-manager/#context","title":"Context","text":"<p>Users reported \"device busy\" ALSA/PortAudio errors when multiple TTS requests occurred simultaneously. This happened because multiple threads attempted to access the audio hardware at the same time.</p>"},{"location":"adr/001-audio-queue-manager/#decision","title":"Decision","text":"<p>Implement an <code>AudioQueueManager</code> to serialize audio playback on a single worker thread. All TTS requests are queued (with priority support) and processed sequentially.</p>"},{"location":"adr/001-audio-queue-manager/#consequences","title":"Consequences","text":"<ul> <li>Positive: Eliminates device conflicts, adds priority support, prevents system resource exhaustion from excessive threading.</li> <li>Negative: Slightly increased latency for concurrent requests as they must wait for previous ones to finish.</li> <li>Neutral: Becomes the default behavior, requiring a migration for users who relied on simultaneous (and potentially conflicting) playback.</li> </ul>"},{"location":"api/core/","title":"Kern-API","text":"<p>Diese Seite enth\u00e4lt die Referenz f\u00fcr die Kernkomponenten von <code>text2speech</code>.</p>"},{"location":"api/core/#text2speech","title":"Text2Speech","text":""},{"location":"api/core/#text2speech.text2speech.Text2Speech","title":"<code>text2speech.text2speech.Text2Speech</code>","text":"<p>Text-to-Speech (TTS) class with configurable settings.</p> <p>This class provides text-to-speech functionality using either ElevenLabs or Kokoro model with configuration support via YAML files.</p> Source code in <code>text2speech/text2speech.py</code> <pre><code>class Text2Speech:\n    \"\"\"Text-to-Speech (TTS) class with configurable settings.\n\n    This class provides text-to-speech functionality using either ElevenLabs\n    or Kokoro model with configuration support via YAML files.\n    \"\"\"\n\n    def __init__(\n        self,\n        el_api_key: Optional[str] = None,\n        verbose: Optional[bool] = None,\n        config_path: Optional[str] = None,\n        config: Optional[Config] = None,\n        enable_queue: bool = True,\n        max_queue_size: int = DEFAULT_QUEUE_SIZE,\n        duplicate_timeout: float = DEFAULT_DUPLICATE_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Initialize the Text2Speech instance.\n\n        Args:\n            el_api_key (Optional[str]): API key for ElevenLabs.\n            verbose (Optional[bool]): If True, prints debug info. Overrides config if set.\n            config_path (Optional[str]): Path to config.yaml file.\n            config (Optional[Config]): Pre-loaded Config object.\n            enable_queue (bool): If True, uses AudioQueueManager for thread-safe playback.\n            max_queue_size (int): Maximum queued messages.\n            duplicate_timeout (float): Skip duplicate messages within this window (seconds).\n        \"\"\"\n        # Load configuration\n        if config is not None:\n            self.config = config\n        else:\n            self.config = Config(config_path)\n\n        # Override verbose setting if explicitly provided\n        self._verbose = verbose if verbose is not None else self.config.verbose\n\n        # Setup logging\n        self._setup_logging()\n\n        # Store API key and validate\n        self._el_api_key = el_api_key\n        self._use_elevenlabs = self._validate_elevenlabs_key(el_api_key)\n\n        # Initialize TTS engine\n        self._engine: Optional[TTSEngine] = None\n        self._initialize_tts_engine()\n\n        # Set audio output device from config\n        self._setup_audio_device()\n\n        # Initialize audio queue manager\n        self._enable_queue = enable_queue\n        self._audio_queue: Optional[AudioQueueManager] = None\n\n        if enable_queue:\n            self._audio_queue = AudioQueueManager(\n                tts_callable=self.speak_sync,\n                max_queue_size=max_queue_size,\n                duplicate_timeout=duplicate_timeout,\n                logger=self.logger,\n            )\n            self._audio_queue.start()\n            self.logger.info(\"Audio queue manager enabled\")\n\n    def _setup_logging(self) -&gt; None:\n        \"\"\"Setup logging configuration with sensitive data filter.\"\"\"\n        log_level = self.config.get(\"logging.log_level\", \"INFO\")\n        self.logger = logging.getLogger(\"text2speech\")\n        self.logger.setLevel(getattr(logging, log_level))\n\n        if not self.logger.handlers:\n            formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(formatter)\n            self.logger.addHandler(console_handler)\n\n        # Add sensitive data filter to all handlers\n        sensitive_filter = SensitiveDataFilter()\n        for handler in self.logger.handlers:\n            handler.addFilter(sensitive_filter)\n\n        log_file = self.config.get(\"logging.log_file\")\n        if log_file:\n            file_handler = logging.FileHandler(log_file)\n            file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n            file_handler.addFilter(sensitive_filter)\n            self.logger.addHandler(file_handler)\n\n    def _setup_audio_device(self) -&gt; None:\n        \"\"\"Configure the audio output device.\"\"\"\n        if HAS_SOUNDDEVICE and sd is not None:\n            device_id = self.config.audio_output_device\n            if device_id is not None:\n                try:\n                    sd.default.device[1] = device_id\n                    self.logger.info(f\"Audio output device set to: {device_id}\")\n                except Exception as e:\n                    self.logger.error(f\"Failed to set audio device {device_id}: {e}\")\n                    raise AudioDeviceError(f\"Invalid audio device: {device_id}\") from e\n\n    def _validate_elevenlabs_key(self, api_key: Optional[str]) -&gt; bool:\n        \"\"\"Validate ElevenLabs API key format.\"\"\"\n        if not api_key or not isinstance(api_key, str):\n            return False\n        return api_key.startswith(API_KEY_PREFIX) and len(api_key) &gt;= MIN_API_KEY_LENGTH\n\n    def _initialize_tts_engine(self) -&gt; None:\n        \"\"\"Initialize the TTS engine with fallback.\"\"\"\n        if self._use_elevenlabs:\n            try:\n                model = self.config.get(\"tts.elevenlabs.model\", \"eleven_multilingual_v2\")\n                self._engine = ElevenLabsEngine(api_key=self._el_api_key, model=model)  # type: ignore\n                self.logger.info(\"Initialized ElevenLabs TTS\")\n                return\n            except Exception as e:\n                self.logger.warning(f\"ElevenLabs initialization failed: {e}. Falling back to Kokoro.\")\n\n        try:\n            self._engine = KokoroEngine(lang_code=self.config.kokoro_lang_code)\n            self.logger.info(\"Initialized Kokoro TTS\")\n        except Exception as e:\n            self.logger.error(f\"Failed to initialize any TTS engine: {e}\")\n            raise TTSEngineNotAvailable(\"No TTS engine available\") from e\n\n    def speak(self, text: str, priority: int = 0, blocking: bool = False) -&gt; bool:\n        \"\"\"Queue text for speech synthesis.\n\n        Args:\n            text (str): Text to speak.\n            priority (int): Priority level (0-100).\n            blocking (bool): If True, wait for speech to complete.\n\n        Returns:\n            bool: True if successfully queued/spoken.\n        \"\"\"\n        if self._audio_queue and self._enable_queue:\n            success = self._audio_queue.enqueue(text, priority=priority)\n            if blocking and success:\n                self._wait_for_queue()\n            return success\n\n        if blocking:\n            self.speak_sync(text)\n            return True\n\n        thread = threading.Thread(target=self.speak_sync, args=(text,))\n        thread.start()\n        return True\n\n    def _wait_for_queue(self) -&gt; None:\n        \"\"\"Wait for the audio queue to be empty.\"\"\"\n        import time\n\n        if self._audio_queue:\n            while not self._audio_queue._queue.empty():\n                time.sleep(0.1)\n            # Wait a bit more for the last message to finish playing\n            time.sleep(0.5)\n\n    def speak_sync(self, text: str) -&gt; None:\n        \"\"\"Synchronous TTS call.\n\n        Args:\n            text (str): Text to speak.\n        \"\"\"\n        if not self._engine:\n            self.logger.error(\"No TTS engine initialized\")\n            return\n\n        try:\n            voice = (\n                self.config.kokoro_voice\n                if isinstance(self._engine, KokoroEngine)\n                else self.config.get(\"tts.elevenlabs.voice\", \"Brian\")\n            )\n            speed = self.config.kokoro_speed\n\n            self.logger.debug(f\"Synthesizing: {text[:50]}...\")\n            for _, _, audio in self._engine.synthesize(text, voice=voice, speed=speed):\n                self._play_audio_safely(audio, original_sample_rate=self.config.sample_rate, volume=self.config.audio_volume)\n                if HAS_SOUNDDEVICE and sd:\n                    sd.wait()\n        except Exception as e:\n            self.logger.error(f\"Speech synthesis error: {e}\")\n\n    @staticmethod\n    def _play_audio_safely(\n        audio_tensor: torch.Tensor,\n        original_sample_rate: int = 24000,\n        device: Optional[int] = None,\n        volume: float = DEFAULT_VOLUME,\n    ) -&gt; None:\n        \"\"\"Play audio safely with resampling and volume control.\n\n        Args:\n            audio_tensor (torch.Tensor): Audio data to play.\n            original_sample_rate (int): Sample rate of the audio data.\n            device (Optional[int]): Audio device ID to use.\n            volume (float): Playback volume (0.0 to 1.0).\n        \"\"\"\n        if not HAS_SOUNDDEVICE or sd is None:\n            logging.getLogger(\"text2speech\").warning(\"sounddevice not available, skipping playback\")\n            return\n\n        try:\n            if device is None:\n                device = sd.default.device[1]\n\n            device_info = sd.query_devices(device, \"output\")\n            supported_rate = int(device_info[\"default_samplerate\"])\n\n            if original_sample_rate != supported_rate:\n                resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=supported_rate)\n                audio_tensor = resampler(audio_tensor)\n\n            peak = torch.abs(audio_tensor).max()\n            if peak &gt; 0:\n                audio_tensor = audio_tensor / peak\n            audio_tensor = torch.clamp(audio_tensor * volume, -0.95, 0.95)\n\n            sd.play(audio_tensor.cpu().numpy(), samplerate=supported_rate, device=device)\n        except Exception as e:\n            logging.getLogger(\"text2speech\").error(f\"Audio playback error: {e}\")\n\n    # Deprecated methods\n    def call_text2speech_async(self, text: str) -&gt; threading.Thread:\n        \"\"\"Deprecated: Use speak(blocking=False) instead.\"\"\"\n        warnings.warn(\"call_text2speech_async is deprecated, use speak(blocking=False)\", DeprecationWarning, stacklevel=2)\n        thread = threading.Thread(target=self.speak_sync, args=(text,))\n        thread.start()\n        return thread\n\n    def call_text2speech(self, text: str) -&gt; None:\n        \"\"\"Deprecated: Use speak(blocking=True) instead.\"\"\"\n        warnings.warn(\"call_text2speech is deprecated, use speak(blocking=True)\", DeprecationWarning, stacklevel=2)\n        self.speak_sync(text)\n\n    def shutdown(self, timeout: float = 5.0) -&gt; None:\n        \"\"\"Shutdown the TTS system.\n\n        Args:\n            timeout (float): Maximum seconds to wait for shutdown.\n        \"\"\"\n        if self._audio_queue:\n            self._audio_queue.shutdown(timeout=timeout)\n\n    def __del__(self) -&gt; None:\n        \"\"\"Destructor to ensure cleanup of resources.\"\"\"\n        self.shutdown()\n\n    def set_voice(self, voice: str) -&gt; None:\n        \"\"\"Set the voice for TTS.\n\n        Args:\n            voice (str): Name of the voice to use.\n        \"\"\"\n        self.config.set(\"tts.kokoro.voice\", voice)\n        self.config.set(\"tts.elevenlabs.voice\", voice)\n        self.logger.info(f\"Voice changed to: {voice}\")\n\n    def set_speed(self, speed: float) -&gt; None:\n        \"\"\"Set the speech speed.\n\n        Args:\n            speed (float): Speech speed (0.5 to 2.0).\n        \"\"\"\n        if 0.5 &lt;= speed &lt;= 2.0:\n            self.config.set(\"tts.kokoro.speed\", speed)\n            self.logger.info(f\"Speed changed to: {speed}\")\n        else:\n            self.logger.warning(f\"Speed {speed} out of range (0.5-2.0)\")\n\n    def set_volume(self, volume: float) -&gt; None:\n        \"\"\"Set the playback volume.\n\n        Args:\n            volume (float): Playback volume (0.0 to 1.0).\n        \"\"\"\n        if 0.0 &lt;= volume &lt;= 1.0:\n            self.config.set(\"audio.default_volume\", volume)\n            self.logger.info(f\"Volume changed to: {volume}\")\n        else:\n            self.logger.warning(f\"Volume {volume} out of range (0.0-1.0)\")\n\n    def get_available_devices(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get available audio devices.\n\n        Returns:\n            List[Dict[str, Any]]: List of available audio devices.\n        \"\"\"\n        if HAS_SOUNDDEVICE and sd:\n            return list(sd.query_devices())\n        return []\n\n    def is_using_elevenlabs(self) -&gt; bool:\n        \"\"\"Check if using ElevenLabs.\"\"\"\n        if not self._engine:\n            return False\n        # Handle both real classes and mocks\n        return \"ElevenLabsEngine\" in str(self._engine) or \"ElevenLabsEngine\" in str(type(self._engine))\n\n    def get_queue_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get queue statistics.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing queue statistics.\n        \"\"\"\n        if self._audio_queue:\n            return dict(self._audio_queue.get_stats())\n        return {}\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.__init__","title":"<code>__init__(el_api_key=None, verbose=None, config_path=None, config=None, enable_queue=True, max_queue_size=DEFAULT_QUEUE_SIZE, duplicate_timeout=DEFAULT_DUPLICATE_TIMEOUT)</code>","text":"<p>Initialize the Text2Speech instance.</p> <p>Parameters:</p> Name Type Description Default <code>el_api_key</code> <code>Optional[str]</code> <p>API key for ElevenLabs.</p> <code>None</code> <code>verbose</code> <code>Optional[bool]</code> <p>If True, prints debug info. Overrides config if set.</p> <code>None</code> <code>config_path</code> <code>Optional[str]</code> <p>Path to config.yaml file.</p> <code>None</code> <code>config</code> <code>Optional[Config]</code> <p>Pre-loaded Config object.</p> <code>None</code> <code>enable_queue</code> <code>bool</code> <p>If True, uses AudioQueueManager for thread-safe playback.</p> <code>True</code> <code>max_queue_size</code> <code>int</code> <p>Maximum queued messages.</p> <code>DEFAULT_QUEUE_SIZE</code> <code>duplicate_timeout</code> <code>float</code> <p>Skip duplicate messages within this window (seconds).</p> <code>DEFAULT_DUPLICATE_TIMEOUT</code> Source code in <code>text2speech/text2speech.py</code> <pre><code>def __init__(\n    self,\n    el_api_key: Optional[str] = None,\n    verbose: Optional[bool] = None,\n    config_path: Optional[str] = None,\n    config: Optional[Config] = None,\n    enable_queue: bool = True,\n    max_queue_size: int = DEFAULT_QUEUE_SIZE,\n    duplicate_timeout: float = DEFAULT_DUPLICATE_TIMEOUT,\n) -&gt; None:\n    \"\"\"Initialize the Text2Speech instance.\n\n    Args:\n        el_api_key (Optional[str]): API key for ElevenLabs.\n        verbose (Optional[bool]): If True, prints debug info. Overrides config if set.\n        config_path (Optional[str]): Path to config.yaml file.\n        config (Optional[Config]): Pre-loaded Config object.\n        enable_queue (bool): If True, uses AudioQueueManager for thread-safe playback.\n        max_queue_size (int): Maximum queued messages.\n        duplicate_timeout (float): Skip duplicate messages within this window (seconds).\n    \"\"\"\n    # Load configuration\n    if config is not None:\n        self.config = config\n    else:\n        self.config = Config(config_path)\n\n    # Override verbose setting if explicitly provided\n    self._verbose = verbose if verbose is not None else self.config.verbose\n\n    # Setup logging\n    self._setup_logging()\n\n    # Store API key and validate\n    self._el_api_key = el_api_key\n    self._use_elevenlabs = self._validate_elevenlabs_key(el_api_key)\n\n    # Initialize TTS engine\n    self._engine: Optional[TTSEngine] = None\n    self._initialize_tts_engine()\n\n    # Set audio output device from config\n    self._setup_audio_device()\n\n    # Initialize audio queue manager\n    self._enable_queue = enable_queue\n    self._audio_queue: Optional[AudioQueueManager] = None\n\n    if enable_queue:\n        self._audio_queue = AudioQueueManager(\n            tts_callable=self.speak_sync,\n            max_queue_size=max_queue_size,\n            duplicate_timeout=duplicate_timeout,\n            logger=self.logger,\n        )\n        self._audio_queue.start()\n        self.logger.info(\"Audio queue manager enabled\")\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.speak","title":"<code>speak(text, priority=0, blocking=False)</code>","text":"<p>Queue text for speech synthesis.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to speak.</p> required <code>priority</code> <code>int</code> <p>Priority level (0-100).</p> <code>0</code> <code>blocking</code> <code>bool</code> <p>If True, wait for speech to complete.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successfully queued/spoken.</p> Source code in <code>text2speech/text2speech.py</code> <pre><code>def speak(self, text: str, priority: int = 0, blocking: bool = False) -&gt; bool:\n    \"\"\"Queue text for speech synthesis.\n\n    Args:\n        text (str): Text to speak.\n        priority (int): Priority level (0-100).\n        blocking (bool): If True, wait for speech to complete.\n\n    Returns:\n        bool: True if successfully queued/spoken.\n    \"\"\"\n    if self._audio_queue and self._enable_queue:\n        success = self._audio_queue.enqueue(text, priority=priority)\n        if blocking and success:\n            self._wait_for_queue()\n        return success\n\n    if blocking:\n        self.speak_sync(text)\n        return True\n\n    thread = threading.Thread(target=self.speak_sync, args=(text,))\n    thread.start()\n    return True\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.speak_sync","title":"<code>speak_sync(text)</code>","text":"<p>Synchronous TTS call.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to speak.</p> required Source code in <code>text2speech/text2speech.py</code> <pre><code>def speak_sync(self, text: str) -&gt; None:\n    \"\"\"Synchronous TTS call.\n\n    Args:\n        text (str): Text to speak.\n    \"\"\"\n    if not self._engine:\n        self.logger.error(\"No TTS engine initialized\")\n        return\n\n    try:\n        voice = (\n            self.config.kokoro_voice\n            if isinstance(self._engine, KokoroEngine)\n            else self.config.get(\"tts.elevenlabs.voice\", \"Brian\")\n        )\n        speed = self.config.kokoro_speed\n\n        self.logger.debug(f\"Synthesizing: {text[:50]}...\")\n        for _, _, audio in self._engine.synthesize(text, voice=voice, speed=speed):\n            self._play_audio_safely(audio, original_sample_rate=self.config.sample_rate, volume=self.config.audio_volume)\n            if HAS_SOUNDDEVICE and sd:\n                sd.wait()\n    except Exception as e:\n        self.logger.error(f\"Speech synthesis error: {e}\")\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.set_voice","title":"<code>set_voice(voice)</code>","text":"<p>Set the voice for TTS.</p> <p>Parameters:</p> Name Type Description Default <code>voice</code> <code>str</code> <p>Name of the voice to use.</p> required Source code in <code>text2speech/text2speech.py</code> <pre><code>def set_voice(self, voice: str) -&gt; None:\n    \"\"\"Set the voice for TTS.\n\n    Args:\n        voice (str): Name of the voice to use.\n    \"\"\"\n    self.config.set(\"tts.kokoro.voice\", voice)\n    self.config.set(\"tts.elevenlabs.voice\", voice)\n    self.logger.info(f\"Voice changed to: {voice}\")\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.set_speed","title":"<code>set_speed(speed)</code>","text":"<p>Set the speech speed.</p> <p>Parameters:</p> Name Type Description Default <code>speed</code> <code>float</code> <p>Speech speed (0.5 to 2.0).</p> required Source code in <code>text2speech/text2speech.py</code> <pre><code>def set_speed(self, speed: float) -&gt; None:\n    \"\"\"Set the speech speed.\n\n    Args:\n        speed (float): Speech speed (0.5 to 2.0).\n    \"\"\"\n    if 0.5 &lt;= speed &lt;= 2.0:\n        self.config.set(\"tts.kokoro.speed\", speed)\n        self.logger.info(f\"Speed changed to: {speed}\")\n    else:\n        self.logger.warning(f\"Speed {speed} out of range (0.5-2.0)\")\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.set_volume","title":"<code>set_volume(volume)</code>","text":"<p>Set the playback volume.</p> <p>Parameters:</p> Name Type Description Default <code>volume</code> <code>float</code> <p>Playback volume (0.0 to 1.0).</p> required Source code in <code>text2speech/text2speech.py</code> <pre><code>def set_volume(self, volume: float) -&gt; None:\n    \"\"\"Set the playback volume.\n\n    Args:\n        volume (float): Playback volume (0.0 to 1.0).\n    \"\"\"\n    if 0.0 &lt;= volume &lt;= 1.0:\n        self.config.set(\"audio.default_volume\", volume)\n        self.logger.info(f\"Volume changed to: {volume}\")\n    else:\n        self.logger.warning(f\"Volume {volume} out of range (0.0-1.0)\")\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.shutdown","title":"<code>shutdown(timeout=5.0)</code>","text":"<p>Shutdown the TTS system.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds to wait for shutdown.</p> <code>5.0</code> Source code in <code>text2speech/text2speech.py</code> <pre><code>def shutdown(self, timeout: float = 5.0) -&gt; None:\n    \"\"\"Shutdown the TTS system.\n\n    Args:\n        timeout (float): Maximum seconds to wait for shutdown.\n    \"\"\"\n    if self._audio_queue:\n        self._audio_queue.shutdown(timeout=timeout)\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.get_available_devices","title":"<code>get_available_devices()</code>","text":"<p>Get available audio devices.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of available audio devices.</p> Source code in <code>text2speech/text2speech.py</code> <pre><code>def get_available_devices(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get available audio devices.\n\n    Returns:\n        List[Dict[str, Any]]: List of available audio devices.\n    \"\"\"\n    if HAS_SOUNDDEVICE and sd:\n        return list(sd.query_devices())\n    return []\n</code></pre>"},{"location":"api/core/#text2speech.text2speech.Text2Speech.get_queue_stats","title":"<code>get_queue_stats()</code>","text":"<p>Get queue statistics.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing queue statistics.</p> Source code in <code>text2speech/text2speech.py</code> <pre><code>def get_queue_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get queue statistics.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing queue statistics.\n    \"\"\"\n    if self._audio_queue:\n        return dict(self._audio_queue.get_stats())\n    return {}\n</code></pre>"},{"location":"api/core/#audioqueuemanager","title":"AudioQueueManager","text":""},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager","title":"<code>text2speech.audio_queue.AudioQueueManager</code>","text":"<p>Thread-safe audio queue manager that serializes TTS playback.</p> Features <ul> <li>Single worker thread for sequential audio playback</li> <li>Priority queue for urgent messages</li> <li>Automatic cleanup on shutdown</li> <li>Skip duplicate messages within timeout</li> <li>Non-blocking queueing</li> </ul> Source code in <code>text2speech/audio_queue.py</code> <pre><code>class AudioQueueManager:\n    \"\"\"Thread-safe audio queue manager that serializes TTS playback.\n\n    Features:\n        - Single worker thread for sequential audio playback\n        - Priority queue for urgent messages\n        - Automatic cleanup on shutdown\n        - Skip duplicate messages within timeout\n        - Non-blocking queueing\n    \"\"\"\n\n    def __init__(\n        self,\n        tts_callable: Callable[[str], None],\n        max_queue_size: int = 50,\n        duplicate_timeout: float = 2.0,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the audio queue manager.\n\n        Args:\n            tts_callable: Synchronous function that performs TTS (blocks until done).\n            max_queue_size: Maximum queued messages (older discarded if full).\n            duplicate_timeout: Skip duplicate messages within this window (seconds).\n            logger: Optional logger instance (creates one if None).\n        \"\"\"\n        self._tts_callable: Callable[[str], None] = tts_callable\n        self._max_queue_size: int = max_queue_size\n        self._duplicate_timeout: float = duplicate_timeout\n\n        # Logging\n        self._logger: logging.Logger = logger or logging.getLogger(__name__)\n\n        # Priority queue (uses __lt__ from AudioTask for ordering)\n        self._queue: queue.PriorityQueue[AudioTask] = queue.PriorityQueue(maxsize=max_queue_size)\n\n        # Worker thread\n        self._worker_thread: Optional[threading.Thread] = None\n        self._shutdown_event: threading.Event = threading.Event()\n\n        # Recent messages tracking (for duplicate detection) using TTL cache\n        self._recent_messages: TTLCache[str, float] = TTLCache(maxsize=MAX_RECENT_MESSAGES, ttl=duplicate_timeout)\n        self._recent_lock: threading.Lock = threading.Lock()\n\n        # Statistics\n        self._stats: Dict[str, int] = {\n            \"messages_queued\": 0,\n            \"messages_played\": 0,\n            \"messages_skipped_duplicate\": 0,\n            \"messages_skipped_full\": 0,\n            \"errors\": 0,\n        }\n        self._stats_lock: threading.Lock = threading.Lock()\n\n    def start(self) -&gt; None:\n        \"\"\"Start the worker thread.\"\"\"\n        if self._worker_thread is not None and self._worker_thread.is_alive():\n            self._logger.warning(\"Worker thread already running\")\n            return\n\n        self._shutdown_event.clear()\n        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True, name=\"AudioQueueWorker\")\n        self._worker_thread.start()\n        self._logger.debug(\"Audio queue manager started\")\n\n    def shutdown(self, timeout: float = 5.0) -&gt; None:\n        \"\"\"Stop the worker thread and wait for completion.\n\n        Args:\n            timeout: Maximum seconds to wait for shutdown.\n        \"\"\"\n        if self._worker_thread is None:\n            return\n\n        self._logger.debug(\"Shutting down audio queue manager...\")\n        self._shutdown_event.set()\n\n        # Signal queue to unblock\n        try:\n            # Add a sentinel task to wake up worker\n            self._queue.put_nowait(AudioTask(\"\", priority=-1000))\n        except queue.Full:\n            pass\n\n        self._worker_thread.join(timeout=timeout)\n\n        if self._worker_thread.is_alive():\n            self._logger.warning(\"Worker thread did not shut down cleanly\")\n        else:\n            self._logger.debug(\"Audio queue manager shut down successfully\")\n\n        # Log final stats\n        self._log_statistics()\n\n    def enqueue(self, text: str, priority: int = 0) -&gt; bool:\n        \"\"\"Queue a message for audio playback (non-blocking).\n\n        Args:\n            text: Message to speak.\n            priority: Priority (higher = more urgent, range 0-100).\n\n        Returns:\n            bool: True if queued successfully, False if skipped/failed.\n        \"\"\"\n        if not text or not text.strip():\n            return False\n\n        # Check for duplicates\n        if self._is_duplicate(text):\n            with self._stats_lock:\n                self._stats[\"messages_skipped_duplicate\"] += 1\n            self._logger.debug(f\"Skipped duplicate: {text[:50]}\")\n            return False\n\n        # Create task\n        task = AudioTask(text=text, priority=priority)\n\n        # Try to queue (non-blocking)\n        try:\n            self._queue.put_nowait(task)\n\n            with self._stats_lock:\n                self._stats[\"messages_queued\"] += 1\n\n            # Track recent message\n            self._track_message(text)\n\n            self._logger.debug(f\"Queued (priority={priority}): {text[:50]}\")\n            return True\n\n        except queue.Full:\n            # Queue full - log and skip\n            with self._stats_lock:\n                self._stats[\"messages_skipped_full\"] += 1\n            self._logger.warning(f\"Queue full, skipped: {text[:50]}\")\n            return False\n\n    def clear_queue(self) -&gt; None:\n        \"\"\"Clear all pending messages from queue.\"\"\"\n        cleared = 0\n        try:\n            while True:\n                self._queue.get_nowait()\n                cleared += 1\n        except queue.Empty:\n            pass\n\n        if cleared &gt; 0:\n            self._logger.info(f\"Cleared {cleared} messages from queue\")\n\n    def get_stats(self) -&gt; MappingProxyType[str, int]:\n        \"\"\"Get playback statistics.\"\"\"\n        with self._stats_lock:\n            return MappingProxyType(self._stats.copy())\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Check if worker thread is active.\"\"\"\n        return self._worker_thread is not None and self._worker_thread.is_alive()\n\n    # Private methods\n\n    def _worker_loop(self) -&gt; None:\n        \"\"\"Worker thread main loop.\"\"\"\n        self._logger.debug(\"Worker thread started\")\n\n        while not self._shutdown_event.is_set():\n            try:\n                # Get next task (blocking with timeout)\n                task = self._queue.get(timeout=1.0)\n\n                # Check if shutdown signal or sentinel\n                if self._shutdown_event.is_set() or task.priority == -1000:\n                    self._queue.task_done()\n                    break\n\n                # Play audio (blocking)\n                self._play_audio(task.text)\n\n                # Mark task as done\n                self._queue.task_done()\n\n            except queue.Empty:\n                # Timeout - check shutdown flag and continue\n                continue\n            except Exception as e:\n                self._logger.error(f\"Worker error: {e}\", exc_info=True)\n                with self._stats_lock:\n                    self._stats[\"errors\"] += 1\n\n        self._logger.debug(\"Worker thread exiting\")\n\n    def _play_audio(self, text: str) -&gt; None:\n        \"\"\"Play audio using TTS callable.\n\n        Args:\n            text: Message to speak.\n        \"\"\"\n        try:\n            self._logger.debug(f\"Playing: {text[:50]}\")\n\n            # Call TTS function (blocking)\n            self._tts_callable(text)\n\n            with self._stats_lock:\n                self._stats[\"messages_played\"] += 1\n\n            self._logger.debug(\"Playback complete\")\n\n        except Exception as e:\n            self._logger.error(f\"TTS error: {e}\", exc_info=True)\n            with self._stats_lock:\n                self._stats[\"errors\"] += 1\n\n    def _is_duplicate(self, text: str) -&gt; bool:\n        \"\"\"Check if message is a recent duplicate.\"\"\"\n        with self._recent_lock:\n            # TTL cache handles expiration automatically\n            return text in self._recent_messages\n\n    def _track_message(self, text: str) -&gt; None:\n        \"\"\"Track message to detect duplicates.\"\"\"\n        with self._recent_lock:\n            # TTL cache handles size and expiration automatically\n            self._recent_messages[text] = time.time()\n\n    def _log_statistics(self) -&gt; None:\n        \"\"\"Log playback statistics.\"\"\"\n        stats = self.get_stats()\n        self._logger.info(\n            f\"Audio Queue Stats: \"\n            f\"queued={stats['messages_queued']}, \"\n            f\"played={stats['messages_played']}, \"\n            f\"skipped_dup={stats['messages_skipped_duplicate']}, \"\n            f\"skipped_full={stats['messages_skipped_full']}, \"\n            f\"errors={stats['errors']}\"\n        )\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.__init__","title":"<code>__init__(tts_callable, max_queue_size=50, duplicate_timeout=2.0, logger=None)</code>","text":"<p>Initialize the audio queue manager.</p> <p>Parameters:</p> Name Type Description Default <code>tts_callable</code> <code>Callable[[str], None]</code> <p>Synchronous function that performs TTS (blocks until done).</p> required <code>max_queue_size</code> <code>int</code> <p>Maximum queued messages (older discarded if full).</p> <code>50</code> <code>duplicate_timeout</code> <code>float</code> <p>Skip duplicate messages within this window (seconds).</p> <code>2.0</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance (creates one if None).</p> <code>None</code> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def __init__(\n    self,\n    tts_callable: Callable[[str], None],\n    max_queue_size: int = 50,\n    duplicate_timeout: float = 2.0,\n    logger: Optional[logging.Logger] = None,\n) -&gt; None:\n    \"\"\"Initialize the audio queue manager.\n\n    Args:\n        tts_callable: Synchronous function that performs TTS (blocks until done).\n        max_queue_size: Maximum queued messages (older discarded if full).\n        duplicate_timeout: Skip duplicate messages within this window (seconds).\n        logger: Optional logger instance (creates one if None).\n    \"\"\"\n    self._tts_callable: Callable[[str], None] = tts_callable\n    self._max_queue_size: int = max_queue_size\n    self._duplicate_timeout: float = duplicate_timeout\n\n    # Logging\n    self._logger: logging.Logger = logger or logging.getLogger(__name__)\n\n    # Priority queue (uses __lt__ from AudioTask for ordering)\n    self._queue: queue.PriorityQueue[AudioTask] = queue.PriorityQueue(maxsize=max_queue_size)\n\n    # Worker thread\n    self._worker_thread: Optional[threading.Thread] = None\n    self._shutdown_event: threading.Event = threading.Event()\n\n    # Recent messages tracking (for duplicate detection) using TTL cache\n    self._recent_messages: TTLCache[str, float] = TTLCache(maxsize=MAX_RECENT_MESSAGES, ttl=duplicate_timeout)\n    self._recent_lock: threading.Lock = threading.Lock()\n\n    # Statistics\n    self._stats: Dict[str, int] = {\n        \"messages_queued\": 0,\n        \"messages_played\": 0,\n        \"messages_skipped_duplicate\": 0,\n        \"messages_skipped_full\": 0,\n        \"errors\": 0,\n    }\n    self._stats_lock: threading.Lock = threading.Lock()\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.start","title":"<code>start()</code>","text":"<p>Start the worker thread.</p> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the worker thread.\"\"\"\n    if self._worker_thread is not None and self._worker_thread.is_alive():\n        self._logger.warning(\"Worker thread already running\")\n        return\n\n    self._shutdown_event.clear()\n    self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True, name=\"AudioQueueWorker\")\n    self._worker_thread.start()\n    self._logger.debug(\"Audio queue manager started\")\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.shutdown","title":"<code>shutdown(timeout=5.0)</code>","text":"<p>Stop the worker thread and wait for completion.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds to wait for shutdown.</p> <code>5.0</code> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def shutdown(self, timeout: float = 5.0) -&gt; None:\n    \"\"\"Stop the worker thread and wait for completion.\n\n    Args:\n        timeout: Maximum seconds to wait for shutdown.\n    \"\"\"\n    if self._worker_thread is None:\n        return\n\n    self._logger.debug(\"Shutting down audio queue manager...\")\n    self._shutdown_event.set()\n\n    # Signal queue to unblock\n    try:\n        # Add a sentinel task to wake up worker\n        self._queue.put_nowait(AudioTask(\"\", priority=-1000))\n    except queue.Full:\n        pass\n\n    self._worker_thread.join(timeout=timeout)\n\n    if self._worker_thread.is_alive():\n        self._logger.warning(\"Worker thread did not shut down cleanly\")\n    else:\n        self._logger.debug(\"Audio queue manager shut down successfully\")\n\n    # Log final stats\n    self._log_statistics()\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.enqueue","title":"<code>enqueue(text, priority=0)</code>","text":"<p>Queue a message for audio playback (non-blocking).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Message to speak.</p> required <code>priority</code> <code>int</code> <p>Priority (higher = more urgent, range 0-100).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if queued successfully, False if skipped/failed.</p> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def enqueue(self, text: str, priority: int = 0) -&gt; bool:\n    \"\"\"Queue a message for audio playback (non-blocking).\n\n    Args:\n        text: Message to speak.\n        priority: Priority (higher = more urgent, range 0-100).\n\n    Returns:\n        bool: True if queued successfully, False if skipped/failed.\n    \"\"\"\n    if not text or not text.strip():\n        return False\n\n    # Check for duplicates\n    if self._is_duplicate(text):\n        with self._stats_lock:\n            self._stats[\"messages_skipped_duplicate\"] += 1\n        self._logger.debug(f\"Skipped duplicate: {text[:50]}\")\n        return False\n\n    # Create task\n    task = AudioTask(text=text, priority=priority)\n\n    # Try to queue (non-blocking)\n    try:\n        self._queue.put_nowait(task)\n\n        with self._stats_lock:\n            self._stats[\"messages_queued\"] += 1\n\n        # Track recent message\n        self._track_message(text)\n\n        self._logger.debug(f\"Queued (priority={priority}): {text[:50]}\")\n        return True\n\n    except queue.Full:\n        # Queue full - log and skip\n        with self._stats_lock:\n            self._stats[\"messages_skipped_full\"] += 1\n        self._logger.warning(f\"Queue full, skipped: {text[:50]}\")\n        return False\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.clear_queue","title":"<code>clear_queue()</code>","text":"<p>Clear all pending messages from queue.</p> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def clear_queue(self) -&gt; None:\n    \"\"\"Clear all pending messages from queue.\"\"\"\n    cleared = 0\n    try:\n        while True:\n            self._queue.get_nowait()\n            cleared += 1\n    except queue.Empty:\n        pass\n\n    if cleared &gt; 0:\n        self._logger.info(f\"Cleared {cleared} messages from queue\")\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.get_stats","title":"<code>get_stats()</code>","text":"<p>Get playback statistics.</p> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def get_stats(self) -&gt; MappingProxyType[str, int]:\n    \"\"\"Get playback statistics.\"\"\"\n    with self._stats_lock:\n        return MappingProxyType(self._stats.copy())\n</code></pre>"},{"location":"api/core/#text2speech.audio_queue.AudioQueueManager.is_running","title":"<code>is_running()</code>","text":"<p>Check if worker thread is active.</p> Source code in <code>text2speech/audio_queue.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Check if worker thread is active.\"\"\"\n    return self._worker_thread is not None and self._worker_thread.is_alive()\n</code></pre>"},{"location":"api/core/#config","title":"Config","text":""},{"location":"api/core/#text2speech.config.Config","title":"<code>text2speech.config.Config</code>","text":"<p>Configuration manager for text2speech settings.</p> Source code in <code>text2speech/config.py</code> <pre><code>class Config:\n    \"\"\"Configuration manager for text2speech settings.\"\"\"\n\n    DEFAULT_CONFIG: Dict[str, Any] = {\n        \"audio\": {\n            \"output_device\": None,\n            \"default_volume\": DEFAULT_VOLUME,\n            \"sample_rate\": DEFAULT_SAMPLE_RATE,\n        },\n        \"tts\": {\n            \"engine\": \"kokoro\",\n            \"kokoro\": {\n                \"lang_code\": \"a\",\n                \"voice\": \"af_heart\",\n                \"speed\": DEFAULT_SPEED,\n                \"split_pattern\": r\"\\n+\",\n            },\n            \"elevenlabs\": {\n                \"voice\": \"Brian\",\n                \"model\": \"eleven_multilingual_v2\",\n            },\n        },\n        \"logging\": {\n            \"verbose\": False,\n            \"log_file\": None,\n            \"log_level\": \"INFO\",\n        },\n        \"performance\": {\n            \"use_gpu\": True,\n            \"num_threads\": 1,\n        },\n    }\n\n    def __init__(self, config_path: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize configuration.\n\n        Args:\n            config_path (Optional[str]): Path to config.yaml file. If None, searches in common locations.\n        \"\"\"\n        self._config: Dict[str, Any] = self.DEFAULT_CONFIG.copy()\n\n        if config_path is None:\n            config_path = self._find_config_file()\n\n        if config_path and os.path.exists(config_path):\n            self.load_from_file(config_path)\n\n    def _find_config_file(self) -&gt; Optional[str]:\n        \"\"\"Search for config.yaml in common locations.\n\n        Returns:\n            Path to config file if found, None otherwise.\n        \"\"\"\n        search_paths: List[str] = [\n            \"config.yaml\",\n            \"config.yml\",\n            os.path.expanduser(\"~/.text2speech/config.yaml\"),\n            os.path.expanduser(\"~/.config/text2speech/config.yaml\"),\n            \"/etc/text2speech/config.yaml\",\n        ]\n\n        for path in search_paths:\n            if os.path.exists(path):\n                return path\n\n        return None\n\n    def load_from_file(self, config_path: str) -&gt; None:\n        \"\"\"Load configuration from YAML file.\n\n        Args:\n            config_path (str): Path to the YAML configuration file.\n\n        Raises:\n            FileNotFoundError: If config file doesn't exist.\n            yaml.YAMLError: If config file is invalid YAML.\n        \"\"\"\n        try:\n            with open(config_path, \"r\") as f:\n                user_config: Dict[str, Any] = yaml.safe_load(f) or {}\n\n            # Deep merge user config with defaults\n            self._config = self._deep_merge(self.DEFAULT_CONFIG.copy(), user_config)\n\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n        except yaml.YAMLError as e:\n            raise yaml.YAMLError(f\"Invalid YAML in config file: {e}\")\n\n    @staticmethod\n    def _deep_merge(base: Dict[str, Any], update: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Deep merge two dictionaries.\n\n        Args:\n            base (Dict[str, Any]): Base dictionary.\n            update (Dict[str, Any]): Dictionary with updates to apply.\n\n        Returns:\n            Dict[str, Any]: Merged dictionary.\n        \"\"\"\n        result: Dict[str, Any] = base.copy()\n\n        for key, value in update.items():\n            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n                result[key] = Config._deep_merge(result[key], value)\n            else:\n                result[key] = value\n\n        return result\n\n    def get(self, key_path: str, default: Any = None) -&gt; Any:\n        \"\"\"Get configuration value using dot notation.\n\n        Args:\n            key_path (str): Dot-separated path to config value (e.g., 'audio.output_device').\n            default (Any): Default value if key not found.\n\n        Returns:\n            Any: Configuration value or default.\n        \"\"\"\n        keys: List[str] = key_path.split(\".\")\n        value: Any = self._config\n\n        for key in keys:\n            if isinstance(value, dict) and key in value:\n                value = value[key]\n            else:\n                return default\n\n        return value\n\n    def set(self, key_path: str, value: Any) -&gt; None:\n        \"\"\"Set configuration value using dot notation.\n\n        Args:\n            key_path (str): Dot-separated path to config value (e.g., 'audio.output_device').\n            value (Any): Value to set.\n        \"\"\"\n        keys: List[str] = key_path.split(\".\")\n        config: Dict[str, Any] = self._config\n\n        for key in keys[:-1]:\n            if key not in config:\n                config[key] = {}\n            config = config[key]\n\n        config[keys[-1]] = value\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Get full configuration as dictionary.\n\n        Returns:\n            Complete configuration dictionary.\n        \"\"\"\n        return self._config.copy()\n\n    def save_to_file(self, config_path: str) -&gt; None:\n        \"\"\"Save current configuration to YAML file.\n\n        Args:\n            config_path (str): Path where to save the configuration.\n\n        Raises:\n            ValueError: If path is outside allowed directories.\n        \"\"\"\n        path = Path(config_path).resolve()\n\n        # Restrict to safe directories\n        allowed_prefixes = [\n            Path.home().resolve(),\n            Path.cwd().resolve(),\n            Path(tempfile.gettempdir()).resolve(),\n        ]\n\n        is_allowed = False\n        for prefix in allowed_prefixes:\n            try:\n                path.relative_to(prefix)\n                is_allowed = True\n                break\n            except ValueError:\n                continue\n\n        if not is_allowed:\n            raise ValueError(f\"Config path outside allowed directories: {config_path}\")\n\n        # Create directory if it doesn't exist\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(path, \"w\") as f:\n            yaml.dump(self._config, f, default_flow_style=False, sort_keys=False)\n\n    @property\n    def audio_output_device(self) -&gt; Optional[int]:\n        \"\"\"Get audio output device ID.\"\"\"\n        val = self.get(\"audio.output_device\")\n        return int(val) if val is not None else None\n\n    @property\n    def audio_volume(self) -&gt; float:\n        \"\"\"Get default audio volume.\"\"\"\n        return float(self.get(\"audio.default_volume\", DEFAULT_VOLUME))\n\n    @property\n    def sample_rate(self) -&gt; int:\n        \"\"\"Get sample rate.\"\"\"\n        return int(self.get(\"audio.sample_rate\", DEFAULT_SAMPLE_RATE))\n\n    @property\n    def tts_engine(self) -&gt; str:\n        \"\"\"Get TTS engine name.\"\"\"\n        return str(self.get(\"tts.engine\", \"kokoro\"))\n\n    @property\n    def kokoro_lang_code(self) -&gt; str:\n        \"\"\"Get Kokoro language code.\"\"\"\n        return str(self.get(\"tts.kokoro.lang_code\", \"a\"))\n\n    @property\n    def kokoro_voice(self) -&gt; str:\n        \"\"\"Get Kokoro voice.\"\"\"\n        return str(self.get(\"tts.kokoro.voice\", \"af_heart\"))\n\n    @property\n    def kokoro_speed(self) -&gt; float:\n        \"\"\"Get Kokoro speech speed.\"\"\"\n        return float(self.get(\"tts.kokoro.speed\", DEFAULT_SPEED))\n\n    @property\n    def kokoro_split_pattern(self) -&gt; str:\n        \"\"\"Get Kokoro text split pattern.\"\"\"\n        return str(self.get(\"tts.kokoro.split_pattern\", r\"\\n+\"))\n\n    @property\n    def verbose(self) -&gt; bool:\n        \"\"\"Get verbose logging setting.\"\"\"\n        return bool(self.get(\"logging.verbose\", False))\n\n    @property\n    def use_gpu(self) -&gt; bool:\n        \"\"\"Get GPU usage setting.\"\"\"\n        return bool(self.get(\"performance.use_gpu\", True))\n</code></pre>"},{"location":"api/core/#text2speech.config.Config.audio_output_device","title":"<code>audio_output_device</code>  <code>property</code>","text":"<p>Get audio output device ID.</p>"},{"location":"api/core/#text2speech.config.Config.audio_volume","title":"<code>audio_volume</code>  <code>property</code>","text":"<p>Get default audio volume.</p>"},{"location":"api/core/#text2speech.config.Config.sample_rate","title":"<code>sample_rate</code>  <code>property</code>","text":"<p>Get sample rate.</p>"},{"location":"api/core/#text2speech.config.Config.__init__","title":"<code>__init__(config_path=None)</code>","text":"<p>Initialize configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Path to config.yaml file. If None, searches in common locations.</p> <code>None</code> Source code in <code>text2speech/config.py</code> <pre><code>def __init__(self, config_path: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize configuration.\n\n    Args:\n        config_path (Optional[str]): Path to config.yaml file. If None, searches in common locations.\n    \"\"\"\n    self._config: Dict[str, Any] = self.DEFAULT_CONFIG.copy()\n\n    if config_path is None:\n        config_path = self._find_config_file()\n\n    if config_path and os.path.exists(config_path):\n        self.load_from_file(config_path)\n</code></pre>"},{"location":"api/core/#text2speech.config.Config.load_from_file","title":"<code>load_from_file(config_path)</code>","text":"<p>Load configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If config file doesn't exist.</p> <code>YAMLError</code> <p>If config file is invalid YAML.</p> Source code in <code>text2speech/config.py</code> <pre><code>def load_from_file(self, config_path: str) -&gt; None:\n    \"\"\"Load configuration from YAML file.\n\n    Args:\n        config_path (str): Path to the YAML configuration file.\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist.\n        yaml.YAMLError: If config file is invalid YAML.\n    \"\"\"\n    try:\n        with open(config_path, \"r\") as f:\n            user_config: Dict[str, Any] = yaml.safe_load(f) or {}\n\n        # Deep merge user config with defaults\n        self._config = self._deep_merge(self.DEFAULT_CONFIG.copy(), user_config)\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n    except yaml.YAMLError as e:\n        raise yaml.YAMLError(f\"Invalid YAML in config file: {e}\")\n</code></pre>"},{"location":"api/core/#text2speech.config.Config.get","title":"<code>get(key_path, default=None)</code>","text":"<p>Get configuration value using dot notation.</p> <p>Parameters:</p> Name Type Description Default <code>key_path</code> <code>str</code> <p>Dot-separated path to config value (e.g., 'audio.output_device').</p> required <code>default</code> <code>Any</code> <p>Default value if key not found.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Configuration value or default.</p> Source code in <code>text2speech/config.py</code> <pre><code>def get(self, key_path: str, default: Any = None) -&gt; Any:\n    \"\"\"Get configuration value using dot notation.\n\n    Args:\n        key_path (str): Dot-separated path to config value (e.g., 'audio.output_device').\n        default (Any): Default value if key not found.\n\n    Returns:\n        Any: Configuration value or default.\n    \"\"\"\n    keys: List[str] = key_path.split(\".\")\n    value: Any = self._config\n\n    for key in keys:\n        if isinstance(value, dict) and key in value:\n            value = value[key]\n        else:\n            return default\n\n    return value\n</code></pre>"},{"location":"api/core/#text2speech.config.Config.set","title":"<code>set(key_path, value)</code>","text":"<p>Set configuration value using dot notation.</p> <p>Parameters:</p> Name Type Description Default <code>key_path</code> <code>str</code> <p>Dot-separated path to config value (e.g., 'audio.output_device').</p> required <code>value</code> <code>Any</code> <p>Value to set.</p> required Source code in <code>text2speech/config.py</code> <pre><code>def set(self, key_path: str, value: Any) -&gt; None:\n    \"\"\"Set configuration value using dot notation.\n\n    Args:\n        key_path (str): Dot-separated path to config value (e.g., 'audio.output_device').\n        value (Any): Value to set.\n    \"\"\"\n    keys: List[str] = key_path.split(\".\")\n    config: Dict[str, Any] = self._config\n\n    for key in keys[:-1]:\n        if key not in config:\n            config[key] = {}\n        config = config[key]\n\n    config[keys[-1]] = value\n</code></pre>"},{"location":"api/core/#text2speech.config.Config.save_to_file","title":"<code>save_to_file(config_path)</code>","text":"<p>Save current configuration to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path where to save the configuration.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If path is outside allowed directories.</p> Source code in <code>text2speech/config.py</code> <pre><code>def save_to_file(self, config_path: str) -&gt; None:\n    \"\"\"Save current configuration to YAML file.\n\n    Args:\n        config_path (str): Path where to save the configuration.\n\n    Raises:\n        ValueError: If path is outside allowed directories.\n    \"\"\"\n    path = Path(config_path).resolve()\n\n    # Restrict to safe directories\n    allowed_prefixes = [\n        Path.home().resolve(),\n        Path.cwd().resolve(),\n        Path(tempfile.gettempdir()).resolve(),\n    ]\n\n    is_allowed = False\n    for prefix in allowed_prefixes:\n        try:\n            path.relative_to(prefix)\n            is_allowed = True\n            break\n        except ValueError:\n            continue\n\n    if not is_allowed:\n        raise ValueError(f\"Config path outside allowed directories: {config_path}\")\n\n    # Create directory if it doesn't exist\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"w\") as f:\n        yaml.dump(self._config, f, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"api/engines/","title":"TTS-Engines","text":"<p>Referenz f\u00fcr die verschiedenen Sprachsynthese-Engines.</p>"},{"location":"api/engines/#basis-schnittstelle","title":"Basis-Schnittstelle","text":""},{"location":"api/engines/#text2speech.engines.base.TTSEngine","title":"<code>text2speech.engines.base.TTSEngine</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for TTS engine implementations.</p> Source code in <code>text2speech/engines/base.py</code> <pre><code>@runtime_checkable\nclass TTSEngine(Protocol):\n    \"\"\"Protocol for TTS engine implementations.\"\"\"\n\n    def synthesize(\n        self, text: str, voice: Optional[str] = None, speed: float = 1.0\n    ) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n        \"\"\"Synthesize speech from text.\n\n        Args:\n            text (str): Text to synthesize.\n            voice (Optional[str]): Voice identifier.\n            speed (float): Speech speed multiplier.\n\n        Yields:\n            Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n                Tuples of (graphemes, phonemes, audio_tensor).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/engines/#text2speech.engines.base.TTSEngine.synthesize","title":"<code>synthesize(text, voice=None, speed=1.0)</code>","text":"<p>Synthesize speech from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to synthesize.</p> required <code>voice</code> <code>Optional[str]</code> <p>Voice identifier.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Speech speed multiplier.</p> <code>1.0</code> <p>Yields:</p> Type Description <code>Tuple[Optional[str], Optional[str], Tensor]</code> <p>Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]: Tuples of (graphemes, phonemes, audio_tensor).</p> Source code in <code>text2speech/engines/base.py</code> <pre><code>def synthesize(\n    self, text: str, voice: Optional[str] = None, speed: float = 1.0\n) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n    \"\"\"Synthesize speech from text.\n\n    Args:\n        text (str): Text to synthesize.\n        voice (Optional[str]): Voice identifier.\n        speed (float): Speech speed multiplier.\n\n    Yields:\n        Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n            Tuples of (graphemes, phonemes, audio_tensor).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/engines/#kokoro-engine","title":"Kokoro Engine","text":""},{"location":"api/engines/#text2speech.engines.kokoro.KokoroEngine","title":"<code>text2speech.engines.kokoro.KokoroEngine</code>","text":"<p>TTS engine using the Kokoro model.</p> Source code in <code>text2speech/engines/kokoro.py</code> <pre><code>class KokoroEngine:\n    \"\"\"TTS engine using the Kokoro model.\"\"\"\n\n    def __init__(self, lang_code: str = \"a\"):\n        \"\"\"Initialize Kokoro engine.\n\n        Args:\n            lang_code (str): Language code for the pipeline.\n\n        Raises:\n            ImportError: If kokoro package is not installed.\n            RuntimeError: If pipeline initialization fails.\n        \"\"\"\n        if not HAS_KOKORO or KPipeline is None:\n            raise ImportError(\"kokoro package is not installed\")\n        try:\n            self.pipeline = KPipeline(lang_code=lang_code)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to initialize Kokoro pipeline: {e}\")\n\n    def synthesize(\n        self, text: str, voice: Optional[str] = None, speed: float = 1.0\n    ) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n        \"\"\"Synthesize speech using Kokoro.\n\n        Args:\n            text (str): Text to synthesize.\n            voice (Optional[str]): Voice identifier.\n            speed (float): Speech speed multiplier.\n\n        Yields:\n            Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n                Tuples of (graphemes, phonemes, audio_tensor).\n        \"\"\"\n        # Kokoro pipeline returns a generator\n        generator = self.pipeline(text, voice=voice, speed=speed)\n        for gs, ps, audio in generator:\n            if not isinstance(audio, torch.Tensor):\n                audio = torch.from_numpy(audio)\n            yield gs, ps, audio\n</code></pre>"},{"location":"api/engines/#text2speech.engines.kokoro.KokoroEngine.__init__","title":"<code>__init__(lang_code='a')</code>","text":"<p>Initialize Kokoro engine.</p> <p>Parameters:</p> Name Type Description Default <code>lang_code</code> <code>str</code> <p>Language code for the pipeline.</p> <code>'a'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If kokoro package is not installed.</p> <code>RuntimeError</code> <p>If pipeline initialization fails.</p> Source code in <code>text2speech/engines/kokoro.py</code> <pre><code>def __init__(self, lang_code: str = \"a\"):\n    \"\"\"Initialize Kokoro engine.\n\n    Args:\n        lang_code (str): Language code for the pipeline.\n\n    Raises:\n        ImportError: If kokoro package is not installed.\n        RuntimeError: If pipeline initialization fails.\n    \"\"\"\n    if not HAS_KOKORO or KPipeline is None:\n        raise ImportError(\"kokoro package is not installed\")\n    try:\n        self.pipeline = KPipeline(lang_code=lang_code)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to initialize Kokoro pipeline: {e}\")\n</code></pre>"},{"location":"api/engines/#text2speech.engines.kokoro.KokoroEngine.synthesize","title":"<code>synthesize(text, voice=None, speed=1.0)</code>","text":"<p>Synthesize speech using Kokoro.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to synthesize.</p> required <code>voice</code> <code>Optional[str]</code> <p>Voice identifier.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Speech speed multiplier.</p> <code>1.0</code> <p>Yields:</p> Type Description <code>Tuple[Optional[str], Optional[str], Tensor]</code> <p>Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]: Tuples of (graphemes, phonemes, audio_tensor).</p> Source code in <code>text2speech/engines/kokoro.py</code> <pre><code>def synthesize(\n    self, text: str, voice: Optional[str] = None, speed: float = 1.0\n) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n    \"\"\"Synthesize speech using Kokoro.\n\n    Args:\n        text (str): Text to synthesize.\n        voice (Optional[str]): Voice identifier.\n        speed (float): Speech speed multiplier.\n\n    Yields:\n        Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n            Tuples of (graphemes, phonemes, audio_tensor).\n    \"\"\"\n    # Kokoro pipeline returns a generator\n    generator = self.pipeline(text, voice=voice, speed=speed)\n    for gs, ps, audio in generator:\n        if not isinstance(audio, torch.Tensor):\n            audio = torch.from_numpy(audio)\n        yield gs, ps, audio\n</code></pre>"},{"location":"api/engines/#elevenlabs-engine","title":"ElevenLabs Engine","text":""},{"location":"api/engines/#text2speech.engines.elevenlabs.ElevenLabsEngine","title":"<code>text2speech.engines.elevenlabs.ElevenLabsEngine</code>","text":"<p>TTS engine using ElevenLabs API.</p> Source code in <code>text2speech/engines/elevenlabs.py</code> <pre><code>class ElevenLabsEngine:\n    \"\"\"TTS engine using ElevenLabs API.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"eleven_multilingual_v2\"):\n        \"\"\"Initialize ElevenLabs engine.\n\n        Args:\n            api_key (str): ElevenLabs API key.\n            model (str): Model identifier.\n\n        Raises:\n            ImportError: If elevenlabs package is not installed.\n        \"\"\"\n        if not HAS_ELEVENLABS or ElevenLabs is None:\n            raise ImportError(\"elevenlabs package is not installed\")\n        self.client = ElevenLabs(api_key=api_key)\n        self.model = model\n\n    def synthesize(\n        self, text: str, voice: Optional[str] = None, speed: float = 1.0\n    ) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n        \"\"\"Synthesize speech using ElevenLabs.\n\n        Args:\n            text (str): Text to synthesize.\n            voice (Optional[str]): Voice identifier.\n            speed (float): Speech speed multiplier (currently ignored for ElevenLabs).\n\n        Yields:\n            Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n                Tuples of (graphemes, phonemes, audio_tensor).\n        \"\"\"\n        client: Any = self.client\n        audio_generator = client.generate(text=text, voice=voice or \"Brian\", model=self.model)\n\n        if isinstance(audio_generator, bytes):\n            audio_tensor = self._bytes_to_tensor(audio_generator)\n            yield None, None, audio_tensor\n        else:\n            # Collect bytes from generator\n            audio_bytes = b\"\".join(audio_generator)\n            audio_tensor = self._bytes_to_tensor(audio_bytes)\n            yield None, None, audio_tensor\n\n    def _bytes_to_tensor(self, audio_bytes: bytes) -&gt; torch.Tensor:\n        \"\"\"Convert audio bytes to torch Tensor.\n\n        Args:\n            audio_bytes (bytes): Raw audio data (typically MP3).\n\n        Returns:\n            torch.Tensor: 1D torch Tensor of audio waveform.\n        \"\"\"\n        buffer = io.BytesIO(audio_bytes)\n        try:\n            waveform, _ = torchaudio.load(buffer)\n            # Convert to mono if multi-channel\n            if waveform.shape[0] &gt; 1:\n                waveform = torch.mean(waveform, dim=0, keepdim=True)\n            return cast(torch.Tensor, waveform.squeeze(0))\n        except Exception as e:\n            # If torchaudio fails (e.g. missing codec), this will raise\n            raise RuntimeError(f\"Failed to decode ElevenLabs audio: {e}\")\n</code></pre>"},{"location":"api/engines/#text2speech.engines.elevenlabs.ElevenLabsEngine.__init__","title":"<code>__init__(api_key, model='eleven_multilingual_v2')</code>","text":"<p>Initialize ElevenLabs engine.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>ElevenLabs API key.</p> required <code>model</code> <code>str</code> <p>Model identifier.</p> <code>'eleven_multilingual_v2'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If elevenlabs package is not installed.</p> Source code in <code>text2speech/engines/elevenlabs.py</code> <pre><code>def __init__(self, api_key: str, model: str = \"eleven_multilingual_v2\"):\n    \"\"\"Initialize ElevenLabs engine.\n\n    Args:\n        api_key (str): ElevenLabs API key.\n        model (str): Model identifier.\n\n    Raises:\n        ImportError: If elevenlabs package is not installed.\n    \"\"\"\n    if not HAS_ELEVENLABS or ElevenLabs is None:\n        raise ImportError(\"elevenlabs package is not installed\")\n    self.client = ElevenLabs(api_key=api_key)\n    self.model = model\n</code></pre>"},{"location":"api/engines/#text2speech.engines.elevenlabs.ElevenLabsEngine.synthesize","title":"<code>synthesize(text, voice=None, speed=1.0)</code>","text":"<p>Synthesize speech using ElevenLabs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to synthesize.</p> required <code>voice</code> <code>Optional[str]</code> <p>Voice identifier.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Speech speed multiplier (currently ignored for ElevenLabs).</p> <code>1.0</code> <p>Yields:</p> Type Description <code>Tuple[Optional[str], Optional[str], Tensor]</code> <p>Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]: Tuples of (graphemes, phonemes, audio_tensor).</p> Source code in <code>text2speech/engines/elevenlabs.py</code> <pre><code>def synthesize(\n    self, text: str, voice: Optional[str] = None, speed: float = 1.0\n) -&gt; Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n    \"\"\"Synthesize speech using ElevenLabs.\n\n    Args:\n        text (str): Text to synthesize.\n        voice (Optional[str]): Voice identifier.\n        speed (float): Speech speed multiplier (currently ignored for ElevenLabs).\n\n    Yields:\n        Iterator[Tuple[Optional[str], Optional[str], torch.Tensor]]:\n            Tuples of (graphemes, phonemes, audio_tensor).\n    \"\"\"\n    client: Any = self.client\n    audio_generator = client.generate(text=text, voice=voice or \"Brian\", model=self.model)\n\n    if isinstance(audio_generator, bytes):\n        audio_tensor = self._bytes_to_tensor(audio_generator)\n        yield None, None, audio_tensor\n    else:\n        # Collect bytes from generator\n        audio_bytes = b\"\".join(audio_generator)\n        audio_tensor = self._bytes_to_tensor(audio_bytes)\n        yield None, None, audio_tensor\n</code></pre>"},{"location":"api/utils/","title":"Hilfsprogramme","text":"<p>Referenz f\u00fcr Hilfsprogramme und interne Dienstprogramme.</p>"},{"location":"api/utils/#logging","title":"Logging","text":""},{"location":"api/utils/#text2speech.logging_utils.SensitiveDataFilter","title":"<code>text2speech.logging_utils.SensitiveDataFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Redact sensitive data from logs.</p> Source code in <code>text2speech/logging_utils.py</code> <pre><code>class SensitiveDataFilter(logging.Filter):\n    \"\"\"Redact sensitive data from logs.\"\"\"\n\n    PATTERNS = [\n        (r\"sk_[a-zA-Z0-9]{20,}\", \"sk_***REDACTED***\"),\n        (r'\"api_key\":\\s*\"[^\"]+', '\"api_key\": \"***\"'),\n    ]\n\n    def filter(self, record: logging.LogRecord) -&gt; bool:\n        \"\"\"Filter log records to redact sensitive information.\n\n        Args:\n            record (logging.LogRecord): The log record to filter.\n\n        Returns:\n            bool: Always True, as it modifies the record in-place.\n        \"\"\"\n        if not isinstance(record.msg, str):\n            return True\n\n        msg = record.msg\n        for pattern, replacement in self.PATTERNS:\n            msg = re.sub(pattern, replacement, msg)\n        record.msg = msg\n        return True\n</code></pre>"},{"location":"api/utils/#text2speech.logging_utils.SensitiveDataFilter.filter","title":"<code>filter(record)</code>","text":"<p>Filter log records to redact sensitive information.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>The log record to filter.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always True, as it modifies the record in-place.</p> Source code in <code>text2speech/logging_utils.py</code> <pre><code>def filter(self, record: logging.LogRecord) -&gt; bool:\n    \"\"\"Filter log records to redact sensitive information.\n\n    Args:\n        record (logging.LogRecord): The log record to filter.\n\n    Returns:\n        bool: Always True, as it modifies the record in-place.\n    \"\"\"\n    if not isinstance(record.msg, str):\n        return True\n\n    msg = record.msg\n    for pattern, replacement in self.PATTERNS:\n        msg = re.sub(pattern, replacement, msg)\n    record.msg = msg\n    return True\n</code></pre>"},{"location":"api/utils/#ausnahmen","title":"Ausnahmen","text":""},{"location":"api/utils/#text2speech.exceptions.Text2SpeechError","title":"<code>text2speech.exceptions.Text2SpeechError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for text2speech errors.</p> Source code in <code>text2speech/exceptions.py</code> <pre><code>class Text2SpeechError(Exception):\n    \"\"\"Base exception for text2speech errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/utils/#text2speech.exceptions.TTSEngineNotAvailable","title":"<code>text2speech.exceptions.TTSEngineNotAvailable</code>","text":"<p>               Bases: <code>Text2SpeechError</code></p> <p>Raised when no TTS engine is available.</p> Source code in <code>text2speech/exceptions.py</code> <pre><code>class TTSEngineNotAvailable(Text2SpeechError):\n    \"\"\"Raised when no TTS engine is available.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/utils/#text2speech.exceptions.AudioDeviceError","title":"<code>text2speech.exceptions.AudioDeviceError</code>","text":"<p>               Bases: <code>Text2SpeechError</code></p> <p>Raised when audio device configuration fails.</p> Source code in <code>text2speech/exceptions.py</code> <pre><code>class AudioDeviceError(Text2SpeechError):\n    \"\"\"Raised when audio device configuration fails.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/utils/#text2speech.exceptions.InvalidConfigurationError","title":"<code>text2speech.exceptions.InvalidConfigurationError</code>","text":"<p>               Bases: <code>Text2SpeechError</code></p> <p>Raised when configuration is invalid.</p> Source code in <code>text2speech/exceptions.py</code> <pre><code>class InvalidConfigurationError(Text2SpeechError):\n    \"\"\"Raised when configuration is invalid.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/utils/#konstanten","title":"Konstanten","text":""},{"location":"api/utils/#text2speech.constants","title":"<code>text2speech.constants</code>","text":"<p>Configuration constants for text2speech.</p>"},{"location":"architecture/","title":"Architektur","text":"<p>Diese Seite beschreibt die interne Architektur von <code>text2speech</code>.</p>"},{"location":"architecture/#systemubersicht","title":"System\u00fcbersicht","text":"<p>Das System ist in mehrere Komponenten unterteilt, die zusammenarbeiten, um Text in Sprache umzuwandeln und diese sicher abzuspielen.</p> graph TD     User([Benutzer/Anwendung]) --&gt; T2S[Text2Speech]     T2S --&gt; Config[Config Manager]     T2S --&gt; AQM[Audio Queue Manager]     AQM --&gt; Worker[Worker Thread]     Worker --&gt; Engine{TTS Engine}     Engine --&gt; Kokoro[Kokoro Engine]     Engine --&gt; EL[ElevenLabs Engine]     Kokoro --&gt; Audio[Audio Daten]     EL --&gt; Audio     Audio --&gt; Proc[Audio Processing]     Proc --&gt; Playback[Sounddevice Playback]"},{"location":"architecture/#datenfluss","title":"Datenfluss","text":"<p>Der Datenfluss folgt einer klaren Pipeline, um eine blockierungsfreie Verarbeitung zu erm\u00f6glichen.</p> sequenceDiagram     participant App as Anwendung     participant T2S as Text2Speech     participant AQM as Audio Queue Manager     participant Worker as Worker Thread     participant Engine as TTS Engine     participant Device as Audio Ger\u00e4t      App-&gt;&gt;T2S: speak(\"Hallo\", priority=5)     T2S-&gt;&gt;AQM: enqueue(task)     AQM--&gt;&gt;T2S: Erfolg (bool)     T2S--&gt;&gt;App: R\u00fcckgabe      loop Worker Loop         Worker-&gt;&gt;AQM: get_task()         AQM--&gt;&gt;Worker: task         Worker-&gt;&gt;Engine: synthesize(text)         Engine--&gt;&gt;Worker: audio_tensor         Worker-&gt;&gt;Device: play(audio)         Device--&gt;&gt;Worker: fertig     end"},{"location":"architecture/#komponentendetails","title":"Komponentendetails","text":""},{"location":"architecture/#text2speech","title":"Text2Speech","text":"<p>Die Haupt-Einstiegsklasse. Sie koordiniert die Initialisierung der Engines, l\u00e4dt die Konfiguration und stellt die \u00f6ffentliche API bereit.</p>"},{"location":"architecture/#audioqueuemanager","title":"AudioQueueManager","text":"<p>Ein thread-sicherer Manager, der eine Priorit\u00e4tswarteschlange verwendet. Er stellt sicher, dass Audio-Anfragen nacheinander verarbeitet werden, was besonders wichtig ist, um Hardware-Konflikte bei ALSA oder PortAudio zu vermeiden.</p>"},{"location":"architecture/#tts-engines","title":"TTS Engines","text":"<p>Wir unterst\u00fctzen zwei Haupt-Backends: 1. Kokoro Engine: Lokal ausgef\u00fchrtes, hocheffizientes Modell (82M Parameter). 2. ElevenLabs Engine: Cloud-basiertes Backend f\u00fcr High-End-Sprachsynthese (Legacy-Support).</p>"},{"location":"architecture/#audio-processing","title":"Audio Processing","text":"<p>Bevor die Audiodaten an die Hardware gesendet werden, durchlaufen sie eine Verarbeitungskette: - Resampling: Anpassung an die vom Ger\u00e4t unterst\u00fctzte Abtastrate. - Normalisierung: Anpassung der Lautst\u00e4rke. - Clamping: Verhindern von \u00dcbersteuerungen.</p>"},{"location":"development/contributing/","title":"Beitragen","text":"<p>Vielen Dank, dass Sie zum <code>text2speech</code>-Projekt beitragen m\u00f6chten!</p>"},{"location":"development/contributing/#entwicklungs-setup","title":"Entwicklungs-Setup","text":"<ol> <li>Repository klonen</li> <li>Virtuelle Umgebung erstellen</li> <li>Abh\u00e4ngigkeiten installieren:    <pre><code>pip install -e \".[dev]\"\npre-commit install\n</code></pre></li> </ol>"},{"location":"development/contributing/#pull-request-prozess","title":"Pull Request Prozess","text":"<ol> <li>Erstellen Sie einen neuen Branch f\u00fcr Ihr Feature oder Ihren Bugfix.</li> <li>Schreiben Sie Tests f\u00fcr Ihre \u00c4nderungen.</li> <li>Stellen Sie sicher, dass alle Tests bestehen (<code>pytest</code>).</li> <li>\u00dcberpr\u00fcfen Sie die Code-Qualit\u00e4t (<code>black</code>, <code>ruff</code>, <code>mypy</code>).</li> <li>Achten Sie auf eine vollst\u00e4ndige Dokumentation der neuen APIs (Google-Style).</li> <li>Senden Sie Ihren Pull Request.</li> </ol>"},{"location":"development/contributing/#konventionelle-commits","title":"Konventionelle Commits","text":"<p>Wir verwenden Conventional Commits, um automatisierte Changelogs zu generieren.</p> <p>Beispiele: - <code>feat: Unterst\u00fctzung f\u00fcr neue Sprache hinzuf\u00fcgen</code> - <code>fix: Behebung eines Speicherlecks in der Queue</code> - <code>docs: Aktualisierung der API-Referenz</code></p>"},{"location":"development/style-guide/","title":"Docstring Style Guide","text":"<p>Dieses Projekt folgt dem Google Python Style Guide f\u00fcr Docstrings. Dies stellt sicher, dass die Dokumentation konsistent ist und von Tools wie <code>mkdocstrings</code> korrekt verarbeitet werden kann.</p>"},{"location":"development/style-guide/#grundformat","title":"Grundformat","text":"<p>Ein Docstring sollte eine kurze Zusammenfassung enthalten, gefolgt von einer detaillierteren Beschreibung (falls erforderlich), den Argumenten, R\u00fcckgabewerten und ausgel\u00f6sten Ausnahmen.</p> <pre><code>def funktion(name: str, alter: int = 0) -&gt; bool:\n    \"\"\"Kurze Zusammenfassung in einem Satz.\n\n    Eine detailliertere Beschreibung, die erkl\u00e4rt, was die Funktion macht\n    und warum man sie verwenden sollte.\n\n    Args:\n        name (str): Der Name der Person.\n        alter (int): Das Alter der Person (Standard ist 0).\n\n    Returns:\n        bool: True, wenn der Vorgang erfolgreich war, andernfalls False.\n\n    Raises:\n        ValueError: Wenn der Name leer ist.\n    \"\"\"\n    if not name:\n        raise ValueError(\"Name darf nicht leer sein\")\n    return True\n</code></pre>"},{"location":"development/style-guide/#klassen","title":"Klassen","text":"<p>Klassen sollten einen Docstring direkt unter der Klassendefinition haben. Die <code>__init__</code>-Methode sollte ebenfalls dokumentiert werden.</p> <pre><code>class Beispiel:\n    \"\"\"Kurze Zusammenfassung der Klasse.\n\n    L\u00e4ngere Beschreibung der Klasse.\n    \"\"\"\n\n    def __init__(self, wert: int):\n        \"\"\"Initialisiert die Klasse.\n\n        Args:\n            wert (int): Der Initialisierungswert.\n        \"\"\"\n        self.wert = wert\n</code></pre>"},{"location":"development/style-guide/#automatisierung","title":"Automatisierung","text":"<p>Wir verwenden <code>interrogate</code>, um die Abdeckung der Docstrings zu messen. Jede \u00f6ffentliche API muss dokumentiert sein.</p> <pre><code># Pr\u00fcfung der Docstring-Abdeckung\ninterrogate text2speech/\n</code></pre>"},{"location":"en/","title":"text2speech Documentation","text":"<p>Welcome to the text2speech module documentation. This module provides text-to-speech (TTS) functionality using the Kokoro-82M model with advanced, thread-safe audio queue management.</p>"},{"location":"en/#overview","title":"Overview","text":"<p>The text2speech module is designed to provide robust and easy-to-use speech synthesis for robotics applications and other Python projects.</p>"},{"location":"en/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Thread-safe audio queue - Prevents ALSA/PortAudio conflicts via serialized playback.</li> <li>\u2705 High-quality synthesis - Uses the Kokoro-82M model for natural-sounding voices.</li> <li>\u2705 Priority-based control - Urgent messages interrupt normal messages.</li> <li>\u2705 Duplicate detection - Avoids repetition of identical messages within short intervals.</li> <li>\u2705 Flexible configuration system - YAML-based settings for audio, voices, and performance.</li> <li>\u2705 Multilingual - Support for various accents and languages.</li> </ul>"},{"location":"en/#quick-access","title":"Quick Access","text":"Section Description \ud83d\ude80 Getting Started Quick introduction to usage \ud83d\udce6 Installation System requirements and setup \u2699\ufe0f Configuration Customizing the library \ud83d\udcda API Reference Detailed technical documentation \ud83c\udfd7\ufe0f Architecture Insights into internal workings"},{"location":"en/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more details.</p>"},{"location":"en/changelog/","title":"Changelog","text":"<p>All notable changes to this project are automatically documented.</p>"},{"location":"en/configuration/","title":"Configuration","text":"<p><code>text2speech</code> offers a flexible configuration system based on YAML files.</p>"},{"location":"en/configuration/#loading-a-configuration-file","title":"Loading a Configuration File","text":"<p>By default, the library searches for a <code>config.yaml</code> in the following locations: 1. Current directory 2. <code>~/.text2speech/config.yaml</code> 3. <code>~/.config/text2speech/config.yaml</code> 4. <code>/etc/text2speech/config.yaml</code></p> <p>You can also explicitly provide a path:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech(config_path=\"path/to/my_config.yaml\")\n</code></pre>"},{"location":"en/configuration/#example-configyaml","title":"Example <code>config.yaml</code>","text":"<p>Here is a complete example configuration with all major options:</p> <pre><code>audio:\n  output_device: null  # null uses the default device\n  default_volume: 0.8  # Volume from 0.0 to 1.0\n  sample_rate: 24000   # Sample rate (default for Kokoro)\n\ntts:\n  engine: \"kokoro\"     # \"kokoro\" or \"elevenlabs\"\n  kokoro:\n    lang_code: \"a\"     # 'a' for American, 'b' for British\n    voice: \"af_heart\"  # Default voice\n    speed: 1.0         # Speech speed\n\nlogging:\n  verbose: false       # Detailed debug output\n  log_level: \"INFO\"    # INFO, DEBUG, WARNING, ERROR\n\nperformance:\n  use_gpu: true        # Use CUDA if available\n</code></pre>"},{"location":"en/configuration/#audio-settings","title":"Audio Settings","text":""},{"location":"en/configuration/#finding-output-devices","title":"Finding Output Devices","text":"<p>To see available devices and their IDs, you can use the API:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ndevices = tts.get_available_devices()\nfor d in devices:\n    print(f\"ID: {d['index']}, Name: {d['name']}\")\n</code></pre> <p>Enter the desired ID into the <code>config.yaml</code> under <code>audio.output_device</code>.</p>"},{"location":"en/configuration/#tts-settings","title":"TTS Settings","text":""},{"location":"en/configuration/#available-voices-kokoro","title":"Available Voices (Kokoro)","text":"<p>American English (<code>lang_code: \"a\"</code>): - <code>af_heart</code> - Female, warm (default) - <code>af_nicole</code> - Female, professional - <code>am_adam</code> - Male, deep - <code>am_michael</code> - Male, friendly</p> <p>British English (<code>lang_code: \"b\"</code>): - <code>bf_emma</code> - Female, elegant - <code>bm_lewis</code> - Male, refined</p>"},{"location":"en/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Settings are applied in the following order of precedence (highest priority first): 1. Constructor arguments (e.g., <code>Text2Speech(verbose=True)</code>) 2. Explicit method calls (e.g., <code>tts.set_voice(\"am_adam\")</code>) 3. Values from the loaded YAML file 4. Internal defaults</p>"},{"location":"en/getting-started/","title":"Getting Started","text":"<p>In this guide, you will learn how to quickly integrate the <code>text2speech</code> module into your project.</p>"},{"location":"en/getting-started/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use <code>text2speech</code> is via the <code>Text2Speech</code> class. By default, the audio queue is enabled, allowing for non-blocking execution.</p> <pre><code>from text2speech import Text2Speech\n\n# Initialize the TTS system\ntts = Text2Speech()\n\n# Queue messages (non-blocking)\ntts.speak(\"Hello, I am ready for action!\")\ntts.speak(\"This message will play after the first one.\")\n\n# Shut down the system properly\ntts.shutdown()\n</code></pre>"},{"location":"en/getting-started/#priorities-and-blocking","title":"Priorities and Blocking","text":"<p>You can control the priority of messages and decide whether the call should wait until the speech has been output.</p> <pre><code># A high-priority message\ntts.speak(\"Warning: Critical error!\", priority=10)\n\n# Blocking call (waits until the message has finished speaking)\ntts.speak(\"Please wait for this announcement.\", blocking=True)\nprint(\"Message finished!\")\n</code></pre>"},{"location":"en/getting-started/#adjusting-voices-and-volume","title":"Adjusting Voices and Volume","text":"<p>You can change the voice, speed, and volume at runtime.</p> <pre><code># Change voice to a male speaker\ntts.set_voice(\"am_adam\")\n\n# Increase speed (0.5 to 2.0)\ntts.set_speed(1.2)\n\n# Adjust volume (0.0 to 1.0)\ntts.set_volume(0.9)\n\ntts.speak(\"I am now speaking with a different voice.\")\n</code></pre>"},{"location":"en/getting-started/#usage-as-a-context-manager","title":"Usage as a Context Manager","text":"<p>For automatic resource management, using it as a context manager is recommended.</p> <pre><code>from text2speech import Text2Speech\n\nwith Text2Speech() as tts:\n    tts.speak(\"Automatic shutdown after this block.\")\n</code></pre>"},{"location":"en/getting-started/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>You can also use the library directly from the terminal:</p> <pre><code>text2speech \"Hello from the command line\" --voice af_nicole\n</code></pre>"},{"location":"en/installation/","title":"Installation","text":"<p>This guide will walk you through installing <code>text2speech</code> and its dependencies.</p>"},{"location":"en/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: Version 3.9 or higher.</li> <li>Operating Systems: Ubuntu (tested), Windows, macOS.</li> <li>Audio: A system with a working audio output device.</li> <li>Memory: Minimum 2GB RAM recommended (4GB for optimal performance).</li> <li>Disk Space: Approx. 500MB for model files.</li> </ul>"},{"location":"en/installation/#installation-from-source","title":"Installation from Source","text":"<p>Clone the repository and install the package in editable mode:</p> <pre><code>git clone https://github.com/dgaida/text2speech.git\ncd text2speech\npip install -e .\n</code></pre> <p>This will automatically install all core dependencies such as <code>torch</code>, <code>kokoro</code>, and <code>sounddevice</code>.</p>"},{"location":"en/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For developers and special use cases:</p>"},{"location":"en/installation/#development-tools","title":"Development Tools","text":"<p><pre><code>pip install -e \".[dev]\"\n</code></pre> This installs tools like <code>pytest</code>, <code>ruff</code>, <code>black</code>, and <code>mypy</code>.</p>"},{"location":"en/installation/#elevenlabs-legacy-support","title":"ElevenLabs (Legacy Support)","text":"<p>If you wish to use the old ElevenLabs backend: <pre><code>pip install elevenlabs\n</code></pre></p>"},{"location":"en/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"en/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<p>You may need to install development libraries for audio support:</p> <pre><code>sudo apt-get update\nsudo apt-get install libasound2-dev libportaudio2\n</code></pre>"},{"location":"en/installation/#windows","title":"Windows","text":"<p>Installation via <code>pip</code> should be sufficient in most cases. Ensure your sound drivers are up to date.</p>"},{"location":"en/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>You can test the installation using the integrated CLI tool:</p> <pre><code>text2speech \"Installation tested successfully.\"\n</code></pre> <p>If you hear a voice, the system is correctly configured.</p>"},{"location":"en/metrics/","title":"Documentation Metrics","text":"<p>Dieses Dashboard zeigt die aktuelle Qualit\u00e4t und Abdeckung der Dokumentation sowie der Tests.</p>"},{"location":"en/metrics/#summary","title":"\ud83d\udcca Summary","text":"Metric Status Wert Ziel API-Abdeckung \u2705 100% &gt;95% Test Coverage \u2705 98% &gt;90% Build-Status \u2705 Passing - Gebrochene Links \u2705 0 0"},{"location":"en/metrics/#api-documentation-coverage","title":"\ud83d\udcc8 API Documentation Coverage","text":"<p>Die API-Abdeckung wird automatisch mit <code>interrogate</code> gemessen. Sie stellt sicher, dass alle \u00f6ffentlichen Klassen, Methoden und Funktionen korrekt dokumentiert sind.</p> pie title API-Abdeckung (interrogate)     \"Documented\" : 100     \"Undocumented\" : 0"},{"location":"en/metrics/#test-coverage","title":"\ud83e\uddea Test Coverage","text":"<p>Die Test Coverage gibt an, wie viel Prozent des Quellcodes durch automatisierte Tests (Pytest) ausgef\u00fchrt werden.</p> pie title Test Coverage (pytest-cov)     \"Covered\" : 98     \"Not covered\" : 2"},{"location":"en/metrics/#documentation-quality","title":"\ud83d\udee0\ufe0f Documentation Quality","text":"Check Tool Status Google-Style Docstrings mkdocstrings \u2705 Passing Markdown Linting pymarkdown \u2705 Passing Mermaid Diagramme mermaid2 \u2705 Passing Cross-Links mkdocs \u2705 Passing"},{"location":"en/metrics/#changelog-freshness","title":"\ud83d\udd52 Changelog Freshness","text":"<p>Der Changelog wird automatisch bei jedem Release \u00fcber <code>git-cliff</code> aktualisiert, basierend auf den Conventional Commits.</p> <p>Last updated: Februar 2026</p>"},{"location":"en/troubleshooting/","title":"Troubleshooting","text":"<p>Here you can find solutions to common issues.</p>"},{"location":"en/troubleshooting/#no-audio-output","title":"No Audio Output","text":"<p>If you cannot hear any speech output:</p> <ol> <li>Check Device List:    Run the following to see if your device is detected:    <pre><code>python -c \"import sounddevice as sd; print(sd.query_devices())\"\n</code></pre></li> <li>Test Default Device:    Ensure that your operating system has the correct default output device selected.</li> <li>Check Configuration:    Check your <code>config.yaml</code> to see if <code>audio.output_device</code> is set to the correct ID or to <code>null</code>.</li> <li>Volume:    Check <code>audio.default_volume</code> and the system volume.</li> </ol>"},{"location":"en/troubleshooting/#alsaportaudio-errors-linux","title":"ALSA/PortAudio Errors (Linux)","text":"<p>If errors like <code>ALSA lib pcm.c:8432:(snd_pcm_recover) underrun occurred</code> occur:</p> <ul> <li>This is often a sign of resource conflicts. Ensure that you are using the <code>AudioQueueManager</code> functionality (enabled by default), which serializes access to the sound card.</li> <li>Install the necessary libraries: <code>sudo apt-get install libasound2-dev libportaudio2</code>.</li> </ul>"},{"location":"en/troubleshooting/#kokoro-model-does-not-load","title":"Kokoro Model Does Not Load","text":"<ul> <li>Internet Connection: The model is downloaded from Hugging Face on the first start. Ensure you have an internet connection.</li> <li>Disk Space: Ensure there is enough disk space (~500MB) available.</li> <li>PyTorch: If CUDA errors occur, check your PyTorch installation:   <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre>   Set <code>performance.use_gpu: false</code> in <code>config.yaml</code> if you do not have a compatible GPU.</li> </ul>"},{"location":"en/troubleshooting/#slow-speech-synthesis","title":"Slow Speech Synthesis","text":"<ul> <li>Use GPU: Ensure that <code>use_gpu: true</code> is configured and that an NVIDIA GPU with installed drivers is available.</li> <li>CPU Threads: If necessary, increase the number of threads in the configuration under <code>performance.num_threads</code>.</li> </ul>"},{"location":"en/troubleshooting/#elevenlabs-authentication-errors","title":"ElevenLabs Authentication Errors","text":"<ul> <li>Ensure that your API key starts with <code>sk_</code>.</li> <li>Check your ElevenLabs account balance.</li> <li>ElevenLabs is only used if a valid API key is provided and the engine field is set to <code>elevenlabs</code>.</li> </ul>"},{"location":"en/api/core/","title":"Core API","text":"<p>This page contains the reference for the core components of <code>text2speech</code>.</p>"},{"location":"en/api/engines/","title":"TTS Engines","text":"<p>Reference for the various speech synthesis engines.</p>"},{"location":"en/api/engines/#base-interface","title":"Base Interface","text":""},{"location":"en/api/utils/","title":"Utilities","text":"<p>Referenz f\u00fcr Utilities und interne Dienstprogramme.</p>"},{"location":"en/api/utils/#exceptions","title":"Exceptions","text":""},{"location":"en/api/utils/#constants","title":"Constants","text":""},{"location":"en/architecture/","title":"Architecture","text":"<p>This page describes the internal architecture of <code>text2speech</code>.</p>"},{"location":"en/architecture/#system-overview","title":"System Overview","text":"<p>The system is divided into several components that work together to convert text to speech and play it safely.</p> graph TD     User([User/Application]) --&gt; T2S[Text2Speech]     T2S --&gt; Config[Config Manager]     T2S --&gt; AQM[Audio Queue Manager]     AQM --&gt; Worker[Worker Thread]     Worker --&gt; Engine{TTS Engine}     Engine --&gt; Kokoro[Kokoro Engine]     Engine --&gt; EL[ElevenLabs Engine]     Kokoro --&gt; Audio[Audio Data]     EL --&gt; Audio     Audio --&gt; Proc[Audio Processing]     Proc --&gt; Playback[Sounddevice Playback]"},{"location":"en/architecture/#data-flow","title":"Data Flow","text":"<p>The data flow follows a clear pipeline to enable non-blocking processing.</p> sequenceDiagram     participant App as Application     participant T2S as Text2Speech     participant AQM as Audio Queue Manager     participant Worker as Worker Thread     participant Engine as TTS Engine     participant Device as Audio Device      App-&gt;&gt;T2S: speak(\"Hello\", priority=5)     T2S-&gt;&gt;AQM: enqueue(task)     AQM--&gt;&gt;T2S: Success (bool)     T2S--&gt;&gt;App: Return      loop Worker Loop         Worker-&gt;&gt;AQM: get_task()         AQM--&gt;&gt;Worker: task         Worker-&gt;&gt;Engine: synthesize(text)         Engine--&gt;&gt;Worker: audio_tensor         Worker-&gt;&gt;Device: play(audio)         Device--&gt;&gt;Worker: finished     end"},{"location":"en/architecture/#component-details","title":"Component Details","text":""},{"location":"en/architecture/#text2speech","title":"Text2Speech","text":"<p>The main entry point class. It coordinates the initialization of engines, loads the configuration, and provides the public API.</p>"},{"location":"en/architecture/#audioqueuemanager","title":"AudioQueueManager","text":"<p>A thread-safe manager that uses a priority queue. It ensures that audio requests are processed sequentially, which is particularly important for avoiding hardware conflicts with ALSA or PortAudio.</p>"},{"location":"en/architecture/#tts-engines","title":"TTS Engines","text":"<p>We support two main backends: 1. Kokoro Engine: Locally executed, highly efficient model (82M parameters). 2. ElevenLabs Engine: Cloud-based backend for high-end speech synthesis (legacy support).</p>"},{"location":"en/architecture/#audio-processing","title":"Audio Processing","text":"<p>Before the audio data is sent to the hardware, it passes through a processing chain: - Resampling: Adjustment to the sample rate supported by the device. - Normalization: Adjustment of the volume. - Clamping: Prevention of clipping.</p>"},{"location":"en/development/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to the <code>text2speech</code> project!</p>"},{"location":"en/development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository</li> <li>Create a virtual environment</li> <li>Install dependencies:    <pre><code>pip install -e \".[dev]\"\npre-commit install\n</code></pre></li> </ol>"},{"location":"en/development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bugfix.</li> <li>Write tests for your changes.</li> <li>Ensure all tests pass (<code>pytest</code>).</li> <li>Check code quality (<code>black</code>, <code>ruff</code>, <code>mypy</code>).</li> <li>Ensure complete documentation for new APIs (Google style).</li> <li>Submit your pull request.</li> </ol>"},{"location":"en/development/contributing/#conventional-commits","title":"Conventional Commits","text":"<p>We use Conventional Commits to generate automated changelogs.</p> <p>Examples: - <code>feat: add support for new language</code> - <code>fix: fix memory leak in queue</code> - <code>docs: update API reference</code></p>"},{"location":"en/development/style-guide/","title":"Docstring Style Guide","text":"<p>This project follows the Google Python Style Guide for docstrings. This ensures that the documentation is consistent and can be correctly processed by tools like <code>mkdocstrings</code>.</p>"},{"location":"en/development/style-guide/#basic-format","title":"Basic Format","text":"<p>A docstring should contain a short summary, followed by a more detailed description (if necessary), arguments, return values, and raised exceptions.</p> <pre><code>def function(name: str, age: int = 0) -&gt; bool:\n    \"\"\"Short summary in one sentence.\n\n    A more detailed description explaining what the function does\n    and why you should use it.\n\n    Args:\n        name (str): The name of the person.\n        age (int): The age of the person (default is 0).\n\n    Returns:\n        bool: True if the operation was successful, otherwise False.\n\n    Raises:\n        ValueError: If the name is empty.\n    \"\"\"\n    if not name:\n        raise ValueError(\"Name cannot be empty\")\n    return True\n</code></pre>"},{"location":"en/development/style-guide/#classes","title":"Classes","text":"<p>Classes should have a docstring immediately below the class definition. The <code>__init__</code> method should also be documented.</p> <pre><code>class Example:\n    \"\"\"Short summary of the class.\n\n    Longer description of the class.\n    \"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Initializes the class.\n\n        Args:\n            value (int): The initialization value.\n        \"\"\"\n        self.value = value\n</code></pre>"},{"location":"en/development/style-guide/#automation","title":"Automation","text":"<p>We use <code>interrogate</code> to measure docstring coverage. Every public API must be documented.</p> <pre><code># Check docstring coverage\ninterrogate text2speech/\n</code></pre>"},{"location":"en/usage/advanced/","title":"Advanced Usage","text":"<p>This covers complex scenarios such as queue management and custom engines.</p>"},{"location":"en/usage/basic/","title":"Basic Usage","text":"<p>Learn how to use <code>text2speech</code> for simple tasks.</p>"},{"location":"en/usage/basic/#simple-speech-output","title":"Simple Speech Output","text":"<pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ntts.speak(\"Hello!\")\n</code></pre>"},{"location":"usage/advanced/","title":"Fortgeschrittene Nutzung","text":"<p>Hier werden komplexe Szenarien wie Queue-Management und benutzerdefinierte Engines behandelt.</p>"},{"location":"usage/basic/","title":"Grundlegende Nutzung","text":"<p>Hier erfahren Sie, wie Sie <code>text2speech</code> f\u00fcr einfache Aufgaben einsetzen.</p>"},{"location":"usage/basic/#einfache-sprachausgabe","title":"Einfache Sprachausgabe","text":"<pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ntts.speak(\"Hallo!\")\n</code></pre>"},{"location":"en/en/","title":"text2speech Documentation","text":"<p>Welcome to the text2speech module documentation. This module provides text-to-speech (TTS) functionality using the Kokoro-82M model with advanced, thread-safe audio queue management.</p>"},{"location":"en/en/#overview","title":"Overview","text":"<p>The text2speech module is designed to provide robust and easy-to-use speech synthesis for robotics applications and other Python projects.</p>"},{"location":"en/en/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Thread-safe audio queue - Prevents ALSA/PortAudio conflicts via serialized playback.</li> <li>\u2705 High-quality synthesis - Uses the Kokoro-82M model for natural-sounding voices.</li> <li>\u2705 Priority-based control - Urgent messages interrupt normal messages.</li> <li>\u2705 Duplicate detection - Avoids repetition of identical messages within short intervals.</li> <li>\u2705 Flexible configuration system - YAML-based settings for audio, voices, and performance.</li> <li>\u2705 Multilingual - Support for various accents and languages.</li> </ul>"},{"location":"en/en/#quick-access","title":"Quick Access","text":"Section Description \ud83d\ude80 Getting Started Quick introduction to usage \ud83d\udce6 Installation System requirements and setup \u2699\ufe0f Configuration Customizing the library \ud83d\udcda API Reference Detailed technical documentation \ud83c\udfd7\ufe0f Architecture Insights into internal workings"},{"location":"en/en/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more details.</p>"},{"location":"en/en/changelog/","title":"Changelog","text":"<p>All notable changes to this project are automatically documented.</p>"},{"location":"en/en/configuration/","title":"Configuration","text":"<p><code>text2speech</code> offers a flexible configuration system based on YAML files.</p>"},{"location":"en/en/configuration/#loading-a-configuration-file","title":"Loading a Configuration File","text":"<p>By default, the library searches for a <code>config.yaml</code> in the following locations: 1. Current directory 2. <code>~/.text2speech/config.yaml</code> 3. <code>~/.config/text2speech/config.yaml</code> 4. <code>/etc/text2speech/config.yaml</code></p> <p>You can also explicitly provide a path:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech(config_path=\"path/to/my_config.yaml\")\n</code></pre>"},{"location":"en/en/configuration/#example-configyaml","title":"Example <code>config.yaml</code>","text":"<p>Here is a complete example configuration with all major options:</p> <pre><code>audio:\n  output_device: null  # null uses the default device\n  default_volume: 0.8  # Volume from 0.0 to 1.0\n  sample_rate: 24000   # Sample rate (default for Kokoro)\n\ntts:\n  engine: \"kokoro\"     # \"kokoro\" or \"elevenlabs\"\n  kokoro:\n    lang_code: \"a\"     # 'a' for American, 'b' for British\n    voice: \"af_heart\"  # Default voice\n    speed: 1.0         # Speech speed\n\nlogging:\n  verbose: false       # Detailed debug output\n  log_level: \"INFO\"    # INFO, DEBUG, WARNING, ERROR\n\nperformance:\n  use_gpu: true        # Use CUDA if available\n</code></pre>"},{"location":"en/en/configuration/#audio-settings","title":"Audio Settings","text":""},{"location":"en/en/configuration/#finding-output-devices","title":"Finding Output Devices","text":"<p>To see available devices and their IDs, you can use the API:</p> <pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ndevices = tts.get_available_devices()\nfor d in devices:\n    print(f\"ID: {d['index']}, Name: {d['name']}\")\n</code></pre> <p>Enter the desired ID into the <code>config.yaml</code> under <code>audio.output_device</code>.</p>"},{"location":"en/en/configuration/#tts-settings","title":"TTS Settings","text":""},{"location":"en/en/configuration/#available-voices-kokoro","title":"Available Voices (Kokoro)","text":"<p>American English (<code>lang_code: \"a\"</code>): - <code>af_heart</code> - Female, warm (default) - <code>af_nicole</code> - Female, professional - <code>am_adam</code> - Male, deep - <code>am_michael</code> - Male, friendly</p> <p>British English (<code>lang_code: \"b\"</code>): - <code>bf_emma</code> - Female, elegant - <code>bm_lewis</code> - Male, refined</p>"},{"location":"en/en/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Settings are applied in the following order of precedence (highest priority first): 1. Constructor arguments (e.g., <code>Text2Speech(verbose=True)</code>) 2. Explicit method calls (e.g., <code>tts.set_voice(\"am_adam\")</code>) 3. Values from the loaded YAML file 4. Internal defaults</p>"},{"location":"en/en/getting-started/","title":"Getting Started","text":"<p>In this guide, you will learn how to quickly integrate the <code>text2speech</code> module into your project.</p>"},{"location":"en/en/getting-started/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use <code>text2speech</code> is via the <code>Text2Speech</code> class. By default, the audio queue is enabled, allowing for non-blocking execution.</p> <pre><code>from text2speech import Text2Speech\n\n# Initialize the TTS system\ntts = Text2Speech()\n\n# Queue messages (non-blocking)\ntts.speak(\"Hello, I am ready for action!\")\ntts.speak(\"This message will play after the first one.\")\n\n# Shut down the system properly\ntts.shutdown()\n</code></pre>"},{"location":"en/en/getting-started/#priorities-and-blocking","title":"Priorities and Blocking","text":"<p>You can control the priority of messages and decide whether the call should wait until the speech has been output.</p> <pre><code># A high-priority message\ntts.speak(\"Warning: Critical error!\", priority=10)\n\n# Blocking call (waits until the message has finished speaking)\ntts.speak(\"Please wait for this announcement.\", blocking=True)\nprint(\"Message finished!\")\n</code></pre>"},{"location":"en/en/getting-started/#adjusting-voices-and-volume","title":"Adjusting Voices and Volume","text":"<p>You can change the voice, speed, and volume at runtime.</p> <pre><code># Change voice to a male speaker\ntts.set_voice(\"am_adam\")\n\n# Increase speed (0.5 to 2.0)\ntts.set_speed(1.2)\n\n# Adjust volume (0.0 to 1.0)\ntts.set_volume(0.9)\n\ntts.speak(\"I am now speaking with a different voice.\")\n</code></pre>"},{"location":"en/en/getting-started/#usage-as-a-context-manager","title":"Usage as a Context Manager","text":"<p>For automatic resource management, using it as a context manager is recommended.</p> <pre><code>from text2speech import Text2Speech\n\nwith Text2Speech() as tts:\n    tts.speak(\"Automatic shutdown after this block.\")\n</code></pre>"},{"location":"en/en/getting-started/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>You can also use the library directly from the terminal:</p> <pre><code>text2speech \"Hello from the command line\" --voice af_nicole\n</code></pre>"},{"location":"en/en/installation/","title":"Installation","text":"<p>This guide will walk you through installing <code>text2speech</code> and its dependencies.</p>"},{"location":"en/en/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: Version 3.9 or higher.</li> <li>Operating Systems: Ubuntu (tested), Windows, macOS.</li> <li>Audio: A system with a working audio output device.</li> <li>Memory: Minimum 2GB RAM recommended (4GB for optimal performance).</li> <li>Disk Space: Approx. 500MB for model files.</li> </ul>"},{"location":"en/en/installation/#installation-from-source","title":"Installation from Source","text":"<p>Clone the repository and install the package in editable mode:</p> <pre><code>git clone https://github.com/dgaida/text2speech.git\ncd text2speech\npip install -e .\n</code></pre> <p>This will automatically install all core dependencies such as <code>torch</code>, <code>kokoro</code>, and <code>sounddevice</code>.</p>"},{"location":"en/en/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For developers and special use cases:</p>"},{"location":"en/en/installation/#development-tools","title":"Development Tools","text":"<p><pre><code>pip install -e \".[dev]\"\n</code></pre> This installs tools like <code>pytest</code>, <code>ruff</code>, <code>black</code>, and <code>mypy</code>.</p>"},{"location":"en/en/installation/#elevenlabs-legacy-support","title":"ElevenLabs (Legacy Support)","text":"<p>If you wish to use the old ElevenLabs backend: <pre><code>pip install elevenlabs\n</code></pre></p>"},{"location":"en/en/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"en/en/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<p>You may need to install development libraries for audio support:</p> <pre><code>sudo apt-get update\nsudo apt-get install libasound2-dev libportaudio2\n</code></pre>"},{"location":"en/en/installation/#windows","title":"Windows","text":"<p>Installation via <code>pip</code> should be sufficient in most cases. Ensure your sound drivers are up to date.</p>"},{"location":"en/en/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>You can test the installation using the integrated CLI tool:</p> <pre><code>text2speech \"Installation tested successfully.\"\n</code></pre> <p>If you hear a voice, the system is correctly configured.</p>"},{"location":"en/en/metrics/","title":"Documentation Metrics","text":"<p>Dieses Dashboard zeigt die aktuelle Qualit\u00e4t und Abdeckung der Dokumentation sowie der Tests.</p>"},{"location":"en/en/metrics/#summary","title":"\ud83d\udcca Summary","text":"Metric Status Wert Ziel API-Abdeckung \u2705 100% &gt;95% Test Coverage \u2705 98% &gt;90% Build-Status \u2705 Passing - Gebrochene Links \u2705 0 0"},{"location":"en/en/metrics/#api-documentation-coverage","title":"\ud83d\udcc8 API Documentation Coverage","text":"<p>Die API-Abdeckung wird automatisch mit <code>interrogate</code> gemessen. Sie stellt sicher, dass alle \u00f6ffentlichen Klassen, Methoden und Funktionen korrekt dokumentiert sind.</p> pie title API-Abdeckung (interrogate)     \"Documented\" : 100     \"Undocumented\" : 0"},{"location":"en/en/metrics/#test-coverage","title":"\ud83e\uddea Test Coverage","text":"<p>Die Test Coverage gibt an, wie viel Prozent des Quellcodes durch automatisierte Tests (Pytest) ausgef\u00fchrt werden.</p> pie title Test Coverage (pytest-cov)     \"Covered\" : 98     \"Not covered\" : 2"},{"location":"en/en/metrics/#documentation-quality","title":"\ud83d\udee0\ufe0f Documentation Quality","text":"Check Tool Status Google-Style Docstrings mkdocstrings \u2705 Passing Markdown Linting pymarkdown \u2705 Passing Mermaid Diagramme mermaid2 \u2705 Passing Cross-Links mkdocs \u2705 Passing"},{"location":"en/en/metrics/#changelog-freshness","title":"\ud83d\udd52 Changelog Freshness","text":"<p>Der Changelog wird automatisch bei jedem Release \u00fcber <code>git-cliff</code> aktualisiert, basierend auf den Conventional Commits.</p> <p>Last updated: Februar 2026</p>"},{"location":"en/en/troubleshooting/","title":"Troubleshooting","text":"<p>Here you can find solutions to common issues.</p>"},{"location":"en/en/troubleshooting/#no-audio-output","title":"No Audio Output","text":"<p>If you cannot hear any speech output:</p> <ol> <li>Check Device List:    Run the following to see if your device is detected:    <pre><code>python -c \"import sounddevice as sd; print(sd.query_devices())\"\n</code></pre></li> <li>Test Default Device:    Ensure that your operating system has the correct default output device selected.</li> <li>Check Configuration:    Check your <code>config.yaml</code> to see if <code>audio.output_device</code> is set to the correct ID or to <code>null</code>.</li> <li>Volume:    Check <code>audio.default_volume</code> and the system volume.</li> </ol>"},{"location":"en/en/troubleshooting/#alsaportaudio-errors-linux","title":"ALSA/PortAudio Errors (Linux)","text":"<p>If errors like <code>ALSA lib pcm.c:8432:(snd_pcm_recover) underrun occurred</code> occur:</p> <ul> <li>This is often a sign of resource conflicts. Ensure that you are using the <code>AudioQueueManager</code> functionality (enabled by default), which serializes access to the sound card.</li> <li>Install the necessary libraries: <code>sudo apt-get install libasound2-dev libportaudio2</code>.</li> </ul>"},{"location":"en/en/troubleshooting/#kokoro-model-does-not-load","title":"Kokoro Model Does Not Load","text":"<ul> <li>Internet Connection: The model is downloaded from Hugging Face on the first start. Ensure you have an internet connection.</li> <li>Disk Space: Ensure there is enough disk space (~500MB) available.</li> <li>PyTorch: If CUDA errors occur, check your PyTorch installation:   <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre>   Set <code>performance.use_gpu: false</code> in <code>config.yaml</code> if you do not have a compatible GPU.</li> </ul>"},{"location":"en/en/troubleshooting/#slow-speech-synthesis","title":"Slow Speech Synthesis","text":"<ul> <li>Use GPU: Ensure that <code>use_gpu: true</code> is configured and that an NVIDIA GPU with installed drivers is available.</li> <li>CPU Threads: If necessary, increase the number of threads in the configuration under <code>performance.num_threads</code>.</li> </ul>"},{"location":"en/en/troubleshooting/#elevenlabs-authentication-errors","title":"ElevenLabs Authentication Errors","text":"<ul> <li>Ensure that your API key starts with <code>sk_</code>.</li> <li>Check your ElevenLabs account balance.</li> <li>ElevenLabs is only used if a valid API key is provided and the engine field is set to <code>elevenlabs</code>.</li> </ul>"},{"location":"en/en/api/core/","title":"Core API","text":"<p>This page contains the reference for the core components of <code>text2speech</code>.</p>"},{"location":"en/en/api/engines/","title":"TTS Engines","text":"<p>Reference for the various speech synthesis engines.</p>"},{"location":"en/en/api/engines/#base-interface","title":"Base Interface","text":""},{"location":"en/en/api/utils/","title":"Utilities","text":"<p>Referenz f\u00fcr Utilities und interne Dienstprogramme.</p>"},{"location":"en/en/api/utils/#exceptions","title":"Exceptions","text":""},{"location":"en/en/api/utils/#constants","title":"Constants","text":""},{"location":"en/en/architecture/","title":"Architecture","text":"<p>This page describes the internal architecture of <code>text2speech</code>.</p>"},{"location":"en/en/architecture/#system-overview","title":"System Overview","text":"<p>The system is divided into several components that work together to convert text to speech and play it safely.</p> graph TD     User([User/Application]) --&gt; T2S[Text2Speech]     T2S --&gt; Config[Config Manager]     T2S --&gt; AQM[Audio Queue Manager]     AQM --&gt; Worker[Worker Thread]     Worker --&gt; Engine{TTS Engine}     Engine --&gt; Kokoro[Kokoro Engine]     Engine --&gt; EL[ElevenLabs Engine]     Kokoro --&gt; Audio[Audio Data]     EL --&gt; Audio     Audio --&gt; Proc[Audio Processing]     Proc --&gt; Playback[Sounddevice Playback]"},{"location":"en/en/architecture/#data-flow","title":"Data Flow","text":"<p>The data flow follows a clear pipeline to enable non-blocking processing.</p> sequenceDiagram     participant App as Application     participant T2S as Text2Speech     participant AQM as Audio Queue Manager     participant Worker as Worker Thread     participant Engine as TTS Engine     participant Device as Audio Device      App-&gt;&gt;T2S: speak(\"Hello\", priority=5)     T2S-&gt;&gt;AQM: enqueue(task)     AQM--&gt;&gt;T2S: Success (bool)     T2S--&gt;&gt;App: Return      loop Worker Loop         Worker-&gt;&gt;AQM: get_task()         AQM--&gt;&gt;Worker: task         Worker-&gt;&gt;Engine: synthesize(text)         Engine--&gt;&gt;Worker: audio_tensor         Worker-&gt;&gt;Device: play(audio)         Device--&gt;&gt;Worker: finished     end"},{"location":"en/en/architecture/#component-details","title":"Component Details","text":""},{"location":"en/en/architecture/#text2speech","title":"Text2Speech","text":"<p>The main entry point class. It coordinates the initialization of engines, loads the configuration, and provides the public API.</p>"},{"location":"en/en/architecture/#audioqueuemanager","title":"AudioQueueManager","text":"<p>A thread-safe manager that uses a priority queue. It ensures that audio requests are processed sequentially, which is particularly important for avoiding hardware conflicts with ALSA or PortAudio.</p>"},{"location":"en/en/architecture/#tts-engines","title":"TTS Engines","text":"<p>We support two main backends: 1. Kokoro Engine: Locally executed, highly efficient model (82M parameters). 2. ElevenLabs Engine: Cloud-based backend for high-end speech synthesis (legacy support).</p>"},{"location":"en/en/architecture/#audio-processing","title":"Audio Processing","text":"<p>Before the audio data is sent to the hardware, it passes through a processing chain: - Resampling: Adjustment to the sample rate supported by the device. - Normalization: Adjustment of the volume. - Clamping: Prevention of clipping.</p>"},{"location":"en/en/development/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to the <code>text2speech</code> project!</p>"},{"location":"en/en/development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository</li> <li>Create a virtual environment</li> <li>Install dependencies:    <pre><code>pip install -e \".[dev]\"\npre-commit install\n</code></pre></li> </ol>"},{"location":"en/en/development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bugfix.</li> <li>Write tests for your changes.</li> <li>Ensure all tests pass (<code>pytest</code>).</li> <li>Check code quality (<code>black</code>, <code>ruff</code>, <code>mypy</code>).</li> <li>Ensure complete documentation for new APIs (Google style).</li> <li>Submit your pull request.</li> </ol>"},{"location":"en/en/development/contributing/#conventional-commits","title":"Conventional Commits","text":"<p>We use Conventional Commits to generate automated changelogs.</p> <p>Examples: - <code>feat: add support for new language</code> - <code>fix: fix memory leak in queue</code> - <code>docs: update API reference</code></p>"},{"location":"en/en/development/style-guide/","title":"Docstring Style Guide","text":"<p>This project follows the Google Python Style Guide for docstrings. This ensures that the documentation is consistent and can be correctly processed by tools like <code>mkdocstrings</code>.</p>"},{"location":"en/en/development/style-guide/#basic-format","title":"Basic Format","text":"<p>A docstring should contain a short summary, followed by a more detailed description (if necessary), arguments, return values, and raised exceptions.</p> <pre><code>def function(name: str, age: int = 0) -&gt; bool:\n    \"\"\"Short summary in one sentence.\n\n    A more detailed description explaining what the function does\n    and why you should use it.\n\n    Args:\n        name (str): The name of the person.\n        age (int): The age of the person (default is 0).\n\n    Returns:\n        bool: True if the operation was successful, otherwise False.\n\n    Raises:\n        ValueError: If the name is empty.\n    \"\"\"\n    if not name:\n        raise ValueError(\"Name cannot be empty\")\n    return True\n</code></pre>"},{"location":"en/en/development/style-guide/#classes","title":"Classes","text":"<p>Classes should have a docstring immediately below the class definition. The <code>__init__</code> method should also be documented.</p> <pre><code>class Example:\n    \"\"\"Short summary of the class.\n\n    Longer description of the class.\n    \"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"Initializes the class.\n\n        Args:\n            value (int): The initialization value.\n        \"\"\"\n        self.value = value\n</code></pre>"},{"location":"en/en/development/style-guide/#automation","title":"Automation","text":"<p>We use <code>interrogate</code> to measure docstring coverage. Every public API must be documented.</p> <pre><code># Check docstring coverage\ninterrogate text2speech/\n</code></pre>"},{"location":"en/en/usage/advanced/","title":"Advanced Usage","text":"<p>This covers complex scenarios such as queue management and custom engines.</p>"},{"location":"en/en/usage/basic/","title":"Basic Usage","text":"<p>Learn how to use <code>text2speech</code> for simple tasks.</p>"},{"location":"en/en/usage/basic/#simple-speech-output","title":"Simple Speech Output","text":"<pre><code>from text2speech import Text2Speech\ntts = Text2Speech()\ntts.speak(\"Hello!\")\n</code></pre>"}]}